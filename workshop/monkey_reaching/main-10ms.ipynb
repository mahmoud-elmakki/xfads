{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81c5c88-d8c2-4b4f-a0ae-30038e78cdb3",
   "metadata": {},
   "source": [
    "# eXponential Family Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/catniplab/xfads/blob/workshop/workshop/monkey_reaching/main.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<br>prev. [Vanderpol oscilator](https://github.com/catniplab/xfads/blob/workshop/workshop/vanderpol/main.ipynb), and [Linear DS](https://github.com/catniplab/xfads/blob/workshop/workshop/linear_ds/main.ipynb).\n",
    "\n",
    "Approximate inference targeted at variational Gaussian state-space models with dense covariance matrix approximations.  For more details, see our paper: [Dowling, Zhao, Park. 2024](https://arxiv.org/abs/2403.01371)\n",
    "\n",
    "This is a walk-through for the minimal functioning of XFADS. We will be building a toy model on the [MC_Maze](https://neurallatents.github.io/datasets.html) dataset as a benchmark, which is a delayed center-out reaching task with barriers.<br>\n",
    "With the adequate configs, you can fit XFADS on different spans of neural data.\n",
    "\n",
    "For an in-depth understanding of the underlying theory of this statistical framework, dive into the paper: [Dowling, Zhao, Park. 2024](https://arxiv.org/abs/2403.01371).For an in-depth understanding of the underlying theory of this statistical framework, dive into the paper: [Dowling, Zhao, Park. 2024](https://arxiv.org/abs/2403.01371)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffa431-270b-422a-83c1-12f08cb7d251",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installation\n",
    "After cloning the repo, and installing anaconda, or miniconda, create the environment by running:<br>\n",
    "`conda env create -f environment.yaml`<br>\n",
    "\n",
    "Add the `xfads` package to the `PYTHONPATH` of the environment<br>\n",
    "`pip install -e .`<br>\n",
    "\n",
    "After running `pip install -e .` for the first time you may need to restart the jupyter kernel.\n",
    "\n",
    "If you are on Google Colab, which is recommended, run the following cell to clone the repo and install the requirements, and you are ready to go!<br>\n",
    "(Since Colab uses sessions anyway, it won't be that useful to use an environment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a75b2c00-a9e2-4cd3-b553-fcfd05cae763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_running_in_colab():\n",
    "    !git clone https://github.com/catniplab/xfads.git\n",
    "    %cd xfads\n",
    "    # install the dependencies\n",
    "    !pip install torch pytorch-lightning scikit-learn hydra-core matplotlib einops\n",
    "    # wrap the XFADS package\n",
    "    !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6276424f-2488-4214-8c83-9e2826657ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random\n",
    "import hydra\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from hydra import compose, initialize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "import pytorch_lightning as lightning\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from mc_utils import *\n",
    "\n",
    "import xfads.utils as utils\n",
    "import xfads.prob_utils as prob_utils\n",
    "from xfads import plot_utils\n",
    "\n",
    "from xfads.ssm_modules.likelihoods import PoissonLikelihood\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianDynamics\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianInitialCondition\n",
    "from xfads.ssm_modules.encoders import LocalEncoderLRMvn, BackwardEncoderLRMvn\n",
    "from xfads.smoothers.lightning_trainers import LightningNonlinearSSM, LightningMonkeyReaching\n",
    "from xfads.smoothers.nonlinear_smoother_causal import NonlinearFilter, LowRankNonlinearStateSpaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77f7cf08-ed8d-4447-9b58-928e633b2665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f8ae4e6-c0d2-41ae-838c-e84ad0aae6bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48c55abd-d3bb-4692-ad46-d1a40918d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"\", job_name=\"monkey_reaching\")\n",
    "\n",
    "cfg = compose(config_name=\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22e19a4a-7edb-4b85-a8bb-314d35836deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1236\n"
     ]
    }
   ],
   "source": [
    "lightning.seed_everything(cfg.seed, workers=True)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f923cdc-62c6-4bbb-916b-bdf4f7d1690d",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "Here we are using the [MC_Maze:](https://dandiarchive.org/dandiset/000128?search=mc_maze&pos=4) macaque primary motor and dorsal premotor cortex spiking activity during delayed reaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "545301b5-38ad-40de-aea4-b7318414e8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH                                                                    SIZE      DONE            DONE% CHECKSUM STATUS          MESSAGE   \n",
      "000128/dandiset.yaml                                                                                             done            updated   \n",
      "000128/sub-Jenkins/sub-Jenkins_ses-full_desc-test_ecephys.nwb           3.4 MB    3.4 MB           100%    ok    done                      \n",
      "000128/sub-Jenkins/sub-Jenkins_ses-full_desc-train_behavior+ecephys.nwb 690.6 MB  690.6 MB         100%    ok    done                      \n",
      "Summary:                                                                694.0 MB  694.0 MB                       3 done          1 updated \n",
      "                                                                                  100.00%                                                  \n",
      "2024-07-12 11:05:12,698 [    INFO] Logs saved in /Users/mahmoud/Library/Logs/dandi-cli/2024.07.12-10.04.09Z-4333.log\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data from DANDI\n",
    "! mkdir data\n",
    "! dandi download DANDI:000128/0.220113.0400 --output-dir ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29759996-eaf5-477d-bd7e-ea7020da9e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shortened 573 trials to prevent overlap.\n",
      "NaNs found in `self.data`. Dropping 18.19% of points to remove NaNs from `trial_data`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splits (train/valid/test) saved into \"data\" folder\n"
     ]
    }
   ],
   "source": [
    "# Arranging the data\n",
    "%run download_data_from_nwb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8942e4f1-d781-4332-aced-53464f768240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training trials: 1721\n",
      "# validation trials: 287\n",
      "# testing trials: 287\n",
      "# neurons: 182\n",
      "# time bins: 90\n",
      "# time bins used for forcasting: 20\n",
      "# predicted time bins: 70\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/data_{split}_{bin_sz_ms}ms.pt'\n",
    "train_data = torch.load(data_path.format(split='train', bin_sz_ms=cfg.bin_sz_ms))\n",
    "val_data = torch.load(data_path.format(split='valid', bin_sz_ms=cfg.bin_sz_ms))\n",
    "test_data = torch.load(data_path.format(split='test', bin_sz_ms=cfg.bin_sz_ms))\n",
    "\n",
    "y_train_obs = train_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_valid_obs = val_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_test_obs = test_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "\n",
    "vel_train = train_data['velocity'].type(torch.float32).to(cfg.data_device)\n",
    "vel_valid = val_data['velocity'].type(torch.float32).to(cfg.data_device)\n",
    "vel_test = test_data['velocity'].type(torch.float32).to(cfg.data_device)\n",
    "\n",
    "y_train_dataset = torch.utils.data.TensorDataset(y_train_obs, vel_train)\n",
    "y_val_dataset = torch.utils.data.TensorDataset(y_valid_obs, vel_valid)\n",
    "y_test_dataset = torch.utils.data.TensorDataset(y_test_obs, vel_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(y_train_dataset, batch_size=cfg.batch_sz, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(y_val_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(y_test_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)\n",
    "\n",
    "# Data dimensions\n",
    "n_train_trials, n_time_bins, n_neurons_obs = y_train_obs.shape\n",
    "n_valid_trials = y_valid_obs.shape[0]\n",
    "n_test_trials = y_test_obs.shape[0]\n",
    "n_time_bins_enc = train_data['n_time_bins_enc']\n",
    "batch_sz_test = list(y_test_obs.shape)[:-1]\n",
    "\n",
    "print(\"# training trials: {0}\".format(n_train_trials))\n",
    "print(\"# validation trials: {0}\".format(n_valid_trials))\n",
    "print(\"# testing trials: {0}\".format(n_test_trials))\n",
    "print(\"# neurons: {0}\".format(n_neurons_obs))\n",
    "print(\"# time bins: {0}\".format(n_time_bins))\n",
    "print(\"# time bins used for forcasting: {0}\".format(cfg.n_bins_bhv))\n",
    "print(\"# predicted time bins: {0}\".format(n_time_bins - cfg.n_bins_bhv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddfd02-8122-448f-ab5a-5d385afc1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of n example trials\n",
    "n_ex_trial = 4\n",
    "ex_trials = np.random.choice(range(0, y_valid.shape[0]), size=n_ex_trial, replace=False)\n",
    "\n",
    "# indices of n example neuron\n",
    "n_ex_neurons = 4\n",
    "ex_neurons = np.random.choice(range(0, y_valid.shape[2]), size=4, replace=False)\n",
    "\n",
    "# top n neurons to plot wihen visualizing trials\n",
    "top_n_neurons = n_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3705019-2737-4f80-aa0a-270bfecfd9d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Structuring the state-space model\n",
    "The modules of the XFADS package are organized in a modular way, allowing the users to change and plug in their own definitions of the classes that structure the elements of the model, i.e. the dynamics function, the likelihood density, the amortization network, etc.\n",
    "\n",
    "The configuration depends on the problem - `dynamics_mod`, `initial_c_pdf`, `likelihood_pdf`, `local_encoder`, and `backward_encoder` can be configured as desired. We include some general classes in `ssm_modules/encoders`, `ssm_modules/likelihoods`, and `ssm_modules/dynamics` that should be sufficient for a wide range of problems.  Below is an example configuration.\n",
    "\n",
    "In each iteration of the variational inference, we need to optimize the parameters of the approximate posterior, which can be quite inefficient, to enable amortized inference for the state-space model, we follow the technique of using a trainable NN, known as an **inference network** or **recognition network**, to predict these parameters from the observed data.\n",
    "\n",
    "A possible drawback of the traditional inference frameworks is that missing observations obstruct inference, i.e. cannot naturally accommodate missing data. But in a state-space graphical model, the latent state posterior should be accessible for every time point even with missing observations. To enable the amortized inference network to process missing observations in a principled way, we decompose the natural parameter update into two additive components: i) a **local encoder**, for current observation, and ii) a **backward encoder**, for future observations. In addition, the separation of local and backward encoders can reduce the complexity of the backward encoder for *L < N* .\n",
    "\n",
    "For a detailed step-by-step development of XFADS, check the Method section of [the paper](https://arxiv.org/abs/2403.01371)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01082bb9-e57d-4a8c-a134-e0eba4946d69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"likelihood pdf\"\"\"\n",
    "H = utils.ReadoutLatentMask(cfg.n_latents, cfg.n_latents_read)\n",
    "readout_fn = nn.Sequential(H, nn.Linear(cfg.n_latents_read, n_neurons_obs))\n",
    "readout_fn[-1].bias.data = prob_utils.estimate_poisson_rate_bias(train_dataloader, cfg.bin_sz)\n",
    "likelihood_pdf = PoissonLikelihood(readout_fn, n_neurons_obs, cfg.bin_sz, device=cfg.device)\n",
    "\n",
    "\"\"\"dynamics module\"\"\"\n",
    "Q_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "dynamics_fn = utils.build_gru_dynamics_function(cfg.n_latents, cfg.n_hidden_dynamics, device=cfg.device)\n",
    "dynamics_mod = DenseGaussianDynamics(dynamics_fn, cfg.n_latents, Q_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"initial condition\"\"\"\n",
    "m_0 = torch.zeros(cfg.n_latents, device=cfg.device)\n",
    "Q_0_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "initial_condition_pdf = DenseGaussianInitialCondition(cfg.n_latents, m_0, Q_0_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"local/backward encoder\"\"\"\n",
    "backward_encoder = BackwardEncoderLRMvn(cfg.n_latents, cfg.n_hidden_backward, cfg.n_latents,\n",
    "                                        rank_local=cfg.rank_local, rank_backward=cfg.rank_backward,\n",
    "                                        device=cfg.device)\n",
    "local_encoder = LocalEncoderLRMvn(cfg.n_latents, n_neurons_obs, cfg.n_hidden_local, cfg.n_latents,\n",
    "                                  rank=cfg.rank_local,\n",
    "                                  device=cfg.device, dropout=cfg.p_local_dropout)\n",
    "nl_filter = NonlinearFilter(dynamics_mod, initial_condition_pdf, device=cfg.device)\n",
    "\n",
    "\"\"\"sequence vae\"\"\"\n",
    "ssm = LowRankNonlinearStateSpaceModel(dynamics_mod, likelihood_pdf, initial_condition_pdf, backward_encoder,\n",
    "                                      local_encoder, nl_filter, device=cfg.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880b37-078b-4024-87b3-0af955b0d937",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2614378d-3bf3-4954-b532-c0e6f37d4f20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type                            | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | ssm  | LowRankNonlinearStateSpaceModel | 604 K  | train\n",
      "-----------------------------------------------------------------\n",
      "604 K     Trainable params\n",
      "0         Non-trainable params\n",
      "604 K     Total params\n",
      "2.420     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4/4 [12:36<00:00,  0.01it/s, v_num=usal]         \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [01:18<00:00,  0.01it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/4 [00:00<?, ?it/s, v_num=usal, r2_valid_enc=-1.00, r2_valid_bhv=-1.00, r2_train_enc=-1.00, time_forward=59.80]        "
     ]
    }
   ],
   "source": [
    "seq_vae = LightningMonkeyReaching(ssm, cfg, n_time_bins_enc, cfg.n_bins_bhv)\n",
    "csv_logger = CSVLogger('logs/smoother/acausal/', name=f'sd_{cfg.seed}_r_y_{cfg.rank_local}_r_b_{cfg.rank_backward}', version='smoother_acausal')\n",
    "ckpt_callback = ModelCheckpoint(save_top_k=3, monitor='r2_valid_enc', mode='max', dirpath='ckpts/smoother/acausal/', save_last=True,\n",
    "                                filename='{epoch:0}_{valid_loss:0.2f}_{r2_valid_enc:0.2f}_{r2_valid_bhv:0.2f}_{valid_bps_enc:0.2f}')\n",
    "\n",
    "trainer = lightning.Trainer(max_epochs=cfg.n_epochs,\n",
    "                            gradient_clip_val=1.0,\n",
    "                            default_root_dir='lightning/',\n",
    "                            callbacks=[ckpt_callback],\n",
    "                            logger=csv_logger,\n",
    "                            )\n",
    "\n",
    "trainer.fit(model=seq_vae, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "torch.save(ckpt_callback.best_model_path, 'ckpts/smoother/acausal/best_model_path.pt')\n",
    "trainer.test(dataloaders=test_dataloader, ckpt_path='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462841f6-257e-4d17-bfb6-47f56c99db05",
   "metadata": {},
   "source": [
    "## Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e68479-0d95-4cb4-8729-3ad1ee949815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"lightning\"\"\"\n",
    "model_ckpt_path = 'ckpts/smoother/acausal/epoch=692_valid_loss=1415.51_r2_valid_enc=0.89_r2_valid_bhv=0.73_valid_bps_enc=0.41.ckpt'\n",
    "seq_vae = LightningMonkeyReaching.load_from_checkpoint(model_ckpt_path, ssm=ssm, cfg=cfg,\n",
    "                                                       n_time_bins_enc=n_time_bins_enc, n_time_bins_bhv=cfg.n_bins_bhv,\n",
    "                                                       strict=False)\n",
    "\"\"\"extract trained ssm from lightning module\"\"\"\n",
    "seq_vae.ssm = seq_vae.ssm.to(cfg.device)\n",
    "seq_vae.ssm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29257cf-ea39-442a-8641-4541acb7720c",
   "metadata": {},
   "source": [
    "## Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641bf13-19fc-4a15-a613-8aec0212f1f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"inference\"\"\"\n",
    "z_s_train = []\n",
    "\n",
    "z_s_test = []\n",
    "z_f_test = []\n",
    "z_p_test = []\n",
    "\n",
    "z_s_valid = []\n",
    "z_f_valid = []\n",
    "z_p_valid = []\n",
    "\n",
    "m_0 = seq_vae.ssm.nl_filter.initial_c_pdf.m_0\n",
    "Q_0 = Fn.softplus(seq_vae.ssm.nl_filter.initial_c_pdf.log_Q_0)\n",
    "#m_0 = seq_vae.ssm.dynamics_mod.initial_c_pdf.m_0\n",
    "#Q_0 = Fn.softplus(seq_vae.ssm.dynamics_mod.initial_c_pdf.log_Q_0)\n",
    "\n",
    "z_ic = m_0 + Q_0.sqrt() * torch.randn(batch_sz_test + [cfg.n_latents], device=cfg.device)\n",
    "z_ic_p = seq_vae.ssm.predict_forward(z_ic, 35 - cfg.n_bins_bhv)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    loss, z, stats = seq_vae.ssm(batch[0], cfg.n_samples)\n",
    "    z_s_train.append(z)\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    z_f, stats = seq_vae.ssm.fast_filter_1_to_T(batch[0], cfg.n_samples)\n",
    "    loss, z, stats = seq_vae.ssm(batch[0], cfg.n_samples)\n",
    "    z_p = seq_vae.ssm.predict_forward(z_f[:, :, cfg.n_bins_bhv], 35-cfg.n_bins_bhv)\n",
    "    z_p = torch.cat([z_f[:, :, :cfg.n_bins_bhv], z_p], dim=2)\n",
    "    z_f_test.append(z_f)\n",
    "    z_p_test.append(z_p)\n",
    "    z_s_test.append(z)\n",
    "    \n",
    "for batch in valid_dataloader:\n",
    "    z_f, stats = seq_vae.ssm.fast_filter_1_to_T(batch[0], cfg.n_samples)\n",
    "    loss, z, stats = seq_vae.ssm(batch[0], cfg.n_samples)\n",
    "    z_p = seq_vae.ssm.predict_forward(z_f[:, :, cfg.n_bins_bhv], cfg.n_samples)\n",
    "    z_p = torch.cat([z_f[:, :, :cfg.n_bins_bhv], z_p], dim=2)\n",
    "    z_f_valid.append(z_f)\n",
    "    z_p_valid.append(z_p)\n",
    "    z_s_valid.append(z)\n",
    "\n",
    "z_s_train = torch.cat(z_s_train, dim=1)\n",
    "\n",
    "z_s_test = torch.cat(z_s_test, dim=1)\n",
    "z_f_test = torch.cat(z_f_test, dim=1)\n",
    "z_p_test = torch.cat(z_p_test, dim=1)\n",
    "\n",
    "z_s_valid = torch.cat(z_s_valid, dim=1)\n",
    "z_f_valid = torch.cat(z_f_valid, dim=1)\n",
    "z_p_valid = torch.cat(z_p_valid, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95850c92-3f85-49e4-a6b0-2af26856a8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"colors\"\"\"\n",
    "blues = cm.get_cmap(\"winter\", z_s_test.shape[0])\n",
    "reds = cm.get_cmap(\"summer\", z_s_test.shape[0])\n",
    "springs = cm.get_cmap(\"spring\", z_s_test.shape[0])\n",
    "\n",
    "color_map_list = [blues, reds, springs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c3e9a-27f1-43e3-923a-0da94000b786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_regimes(y_valid_obs, z_s_test, cfg):\n",
    "    \n",
    "    trial_indcs = np.random.choice(range(0, y_valid_obs.shape[0]), size=4, replace=False)\n",
    "\n",
    "    \"\"\"smoothed\"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(4, 7))\n",
    "\n",
    "        plot_z_samples(fig, axs, z_s_test, trial_indcs, cfg.move_onset, cfg.n_bins_bhv, color_map_list)\n",
    "\n",
    "        fig.suptitle('Smoothed latent trajectories\\n(25 posterior samples from each of the top 3 latents)\\n')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        y_min, _ = plt.ylim()\n",
    "\n",
    "        plt.annotate('movement\\nonset', xy=(cfg.move_onset, y_min), xytext=(cfg.move_onset+3, y_min+0.9*y_min),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 fontsize=8, ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\"filtered\"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(4, 7))\n",
    "\n",
    "        plot_z_samples(fig, axs, z_f_test, trial_indcs, cfg.move_onset, cfg.n_bins_bhv, color_map_list)\n",
    "\n",
    "        fig.suptitle('Filtered latent trajectories\\n(25 posterior samples from each of the top 3 latents)\\n')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        y_min, _ = plt.ylim()\n",
    "\n",
    "        plt.annotate('movement\\nonset', xy=(cfg.move_onset, y_min), xytext=(cfg.move_onset+3, y_min+0.9*y_min),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 fontsize=8, ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \"\"\"predicted\"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        fig, axs = plt.subplots(len(trial_indcs), 1, figsize=(4, 7))\n",
    "        [axs[i].axvline(cfg.n_bins_bhv, linestyle='--', color='red') for i in range(len(trial_indcs))]\n",
    "\n",
    "        plot_z_samples(fig, axs, z_p_test, trial_indcs, cfg.move_onset, cfg.n_bins_bhv, color_map_list)\n",
    "\n",
    "        fig.suptitle('Predicted latent trajectories\\n(25 posterior samples from each of the top 3 latents)\\n')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        y_min, _ = plt.ylim()\n",
    "\n",
    "        plt.annotate('movement\\nonset', xy=(cfg.move_onset, y_min), xytext=(cfg.move_onset+3, y_min+0.9*y_min),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 fontsize=8, ha='center')\n",
    "\n",
    "        plt.annotate('prediction\\nstarts', xy=(cfg.n_bins_bhv, y_min), xytext=(cfg.n_bins_bhv-3, y_min+0.9*y_min),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 fontsize=8, ha='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "plot_regimes(y_valid_obs, z_s_test, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79953bd-f857-4bfd-bd72-f6838fefaac7",
   "metadata": {},
   "source": [
    "## Filtering, Smoothing, and Prediction\n",
    "There is a trick we worked on to conceptualize the approximate smoothing problem as an approximate filtering problem for pseudo-observations that encode a representation of current and future data.\n",
    "\n",
    "We draw inspiration from a quintessential facet of conjugate Bayesian inference: natural parameters of the posterior are a sum-separable combination of the natural parameters of the prior in addition to a data-dependent term. The latter can now be thought of as a data-dependent update to the prior for a conjugate pseudo-observation.\n",
    "\n",
    "Importantly, pseudo-observations defined this way encode the current *and* future observations of the raw data – an essential component for transforming the statistical smoothing problem into an alternative filtering problem.\n",
    "\n",
    "The next figure illustrates the smoothing and predictive performance of XFADS trained on the mc_maze data. To the left of the red line are samples from the posterior during the data window, and to the right of the red line are samples unrolled from $p_{\\theta}(z_t | z_{t-1})$.\n",
    "\n",
    "So, we end up with three regimes:\n",
    "**Filtering**, **Smoothing**, and **Prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba8062-2fec-4c6b-baba-4451487c0ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    trial_list = np.random.choice(range(0, y_valid_obs.shape[0]), size=4, replace=False)\n",
    "\n",
    "    fig, axs = plt.subplots(len(trial_list), 1, figsize=(4, 8))\n",
    "    default_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    \n",
    "    plot_z_2d(fig, axs, z_f_test, trial_list, cfg, color='purple', regime='filtering')\n",
    "    plot_z_2d(fig, axs, z_p_test, trial_list, cfg, color='gray', regime='prediction')\n",
    "    \n",
    "    #fig.suptitle('First 3 latents over time, for all neurons, from 4 test trials \\n (smoothing regime) \\n')\n",
    "    fig.suptitle('filtered vs predicted\\n\\n\\n\\n')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    y_min, _ = plt.ylim()\n",
    "\n",
    "    plt.annotate('move\\nonset', xy=(cfg.move_onset, y_min), xytext=(cfg.move_onset, y_min+0.9*y_min),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "             fontsize=8, ha='center')\n",
    "\n",
    "    plt.annotate('pred\\nstarts', xy=(cfg.n_bins_bhv, y_min), xytext=(cfg.n_bins_bhv-4, y_min+0.9*y_min),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "             fontsize=8, ha='center')\n",
    "    \n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.47, 0.94), shadow=True, ncol=1, fontsize=6)\n",
    "    \n",
    "    # fig.savefig('plots/f_vs_p.png', bbox_inches='tight', transparent=True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d0fdb0-78e7-4ee0-93fe-aaea01262d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    trial_idx = np.random.randint(0, n_valid_trials)\n",
    "    \n",
    "    n_trajectories_to_plot = 9\n",
    "    n_latents_to_plot = 3\n",
    "\n",
    "    fig, axes = plt.subplots(int(np.sqrt(n_trajectories_to_plot)), int(np.sqrt(n_trajectories_to_plot)), figsize=(10, 8), subplot_kw={'projection': '3d'})\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(f'latent trajectories from trial {trial_idx+1}\\n\\n\\n\\n')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        \n",
    "        latent1, latent2, latent3 = np.random.choice(range(0, cfg.n_latents_read), size=n_latents_to_plot, replace=False)\n",
    "        \n",
    "        # Select three latent dimensions for 3D plotting\n",
    "        x = torch.tensor(gaussian_filter1d(torch.mean(z_s_test[:, trial_idx, ..., latent1], dim=0), sigma=4, axis=0))\n",
    "        y = torch.tensor(gaussian_filter1d(torch.mean(z_s_test[:, trial_idx, ..., latent2], dim=0), sigma=4, axis=0))\n",
    "        z = torch.tensor(gaussian_filter1d(torch.mean(z_s_test[:, trial_idx, ..., latent3], dim=0), sigma=4, axis=0))\n",
    "\n",
    "        ax.plot(x, y, z, label='smoothed' if i == 0 else '')\n",
    "        \n",
    "        x_p = torch.tensor(gaussian_filter1d(torch.mean(z_p_test[:, trial_idx, ..., latent1], dim=0), sigma=4, axis=0))\n",
    "        y_p = torch.tensor(gaussian_filter1d(torch.mean(z_p_test[:, trial_idx, ..., latent2], dim=0), sigma=4, axis=0))\n",
    "        z_p = torch.tensor(gaussian_filter1d(torch.mean(z_p_test[:, trial_idx, ..., latent3], dim=0), sigma=4, axis=0))\n",
    "        \n",
    "        ax.tick_params(axis='x', labelsize=6)\n",
    "        ax.tick_params(axis='y', labelsize=6)\n",
    "        ax.tick_params(axis='z', labelsize=6)\n",
    "        \n",
    "        ax.plot(x_p, y_p, z_p, label='predicted' if i == 0 else '')\n",
    "        \n",
    "        ax.set_title(f'latents {latent1+1}, {latent2+1}, {latent3+1}', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.6)\n",
    "    \n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.94), ncol=1, fontsize=7)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bade6-5745-47b2-95df-cbea529e1942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5), subplot_kw={'projection': '3d'})\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    n_latents_to_plot = 3\n",
    "    n_trajectories_to_plot = 3\n",
    "    \n",
    "    trial_idx = np.random.randint(0, z_s_test.shape[1])\n",
    "    \n",
    "    for i in range(n_trajectories_to_plot):\n",
    "        \n",
    "        # Any random 3 latents\n",
    "        latent1, latent2, latent3 = np.random.choice(range(0, cfg.n_latents_read), size=n_latents_to_plot, replace=False)\n",
    "\n",
    "        # Select three latent dimensions for 3D plotting\n",
    "        x = torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(\n",
    "                    z_s_test[:, trial_idx, ..., latent1], dim=0), sigma=4, axis=0))\n",
    "        \n",
    "        y = torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(\n",
    "                    z_s_test[:, trial_idx, ..., latent2], dim=0), sigma=4, axis=0))\n",
    "        \n",
    "        z = torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(\n",
    "                    z_s_test[:, trial_idx, ..., latent3], dim=0), sigma=4, axis=0))\n",
    "\n",
    "        ax.plot(x, y, z, label=f'latents {latent1+1}, {latent2+1}, {latent3+1}')\n",
    "        \n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    fig.suptitle(f'trial {trial_idx+1}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514259d-6a21-4d7c-a6e7-7b7447f1a20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5), subplot_kw={'projection': '3d'})\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    n_latents_to_plot = 3\n",
    "    n_trajectories_to_plot = 3\n",
    "    \n",
    "    trials_inds = np.random.choice(range(0, y_valid_obs.shape[0]), size=n_trajectories_to_plot, replace=False)\n",
    "    \n",
    "    # top 3 latents (not indices)\n",
    "    latent1, latent2, latent3 = [0, 1, 2]\n",
    "    \n",
    "    for i in range(n_trajectories_to_plot):\n",
    "\n",
    "        # Select three latent dimensions for 3D plotting\n",
    "        x = torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(\n",
    "                    z_s_test[:, trials_inds[i], ..., latent1], dim=0), sigma=4, axis=0))\n",
    "        \n",
    "        y = torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(\n",
    "                    z_s_test[:, trials_inds[i], ..., latent2], dim=0), sigma=4, axis=0))\n",
    "        \n",
    "        z = torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(\n",
    "                    z_s_test[:, trials_inds[i], ..., latent3], dim=0), sigma=4, axis=0))\n",
    "\n",
    "        ax.plot(x, y, z, label=f'trial {trials_inds[i]+1}')\n",
    "        \n",
    "    ax.legend(fontsize=8)\n",
    "    ax.view_init(elev=20., azim=-35)\n",
    "    ax.set_xlabel('1st latent')\n",
    "    ax.set_ylabel('2nd latent')\n",
    "    ax.set_zlabel('3rd latent')\n",
    "    \n",
    "    fig.suptitle(f'top 3 principal latents')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ee6924-8e7a-4179-88f0-ae382e1e6d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading the logs\n",
    "\n",
    "logs_path = 'logs/smoother/acausal/sd_1236_r_y_15_r_b_5/smoother_acausal/metrics.csv'\n",
    "metrics = pd.read_csv(logs_path).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f60980-35b9-4533-b21e-f5be99543c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f297fb-6086-4f66-bb4e-329e942aadc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(\n",
    "    metrics['train_loss'].keys(),\n",
    "    metrics['train_loss'].values(),\n",
    "    marker='o', markersize=2, linestyle='-', color='g', alpha=0.6, linewidth=1, label='train')\n",
    "\n",
    "plt.plot(\n",
    "    metrics['valid_loss'].keys(),\n",
    "    metrics['valid_loss'].values(),\n",
    "    marker='o', markersize=2, linestyle='-', color='y', alpha=0.6, linewidth=1, label='valid')\n",
    "\n",
    "# plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (ELBO)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb7239-b85f-4175-8758-9b270869121f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(\n",
    "    list(metrics['r2_train_enc'].keys())[1:],\n",
    "    list(metrics['r2_train_enc'].values())[1:],\n",
    "    marker='o', markersize=2, linestyle='-', color='g', alpha=0.6, linewidth=1, label='train')\n",
    "\n",
    "plt.plot(\n",
    "    list(metrics['r2_valid_enc'].keys())[1:],\n",
    "    list(metrics['r2_valid_enc'].values())[1:],\n",
    "    marker='o', markersize=2, linestyle='-', color='y', alpha=0.6, linewidth=1, label='valid')\n",
    "\n",
    "#plt.title('R2')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('r2')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77ed02-ae7a-4bd2-9ea7-a6c92282888f",
   "metadata": {},
   "source": [
    "Another way to evaluate model performance is the **Bits per Spike (BpS)**. Which is a quantification of how neurons encode information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a884d1-603a-4456-a798-88e36c20b400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(\n",
    "    metrics['train_bps_enc'].keys(),\n",
    "    metrics['train_bps_enc'].values(),\n",
    "    marker='o', markersize=2, linestyle='-', color='g', alpha=0.6, linewidth=1, label='train')\n",
    "\n",
    "plt.plot(\n",
    "    metrics['valid_bps_enc'].keys(),\n",
    "    metrics['valid_bps_enc'].values(),\n",
    "    marker='o', markersize=2, linestyle='-', color='y', alpha=0.6, linewidth=1, label='valid')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('bits per spike')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5c16a-24e8-4d5f-814f-39a51c360bb9",
   "metadata": {},
   "source": [
    "# Generating corresponding observations\n",
    "We can use the likelifood function (observation model) to generate new data (log rates) from the latents. And sample from the rates possion to get the spike counts.\n",
    "\n",
    "Data dimentionality:\\\n",
    "**generated** : sampels x trials x time bins x neurons\\\n",
    "sampels = samples of the latents each of which used to generate the data\\\n",
    "**real**: trials x time bins x neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b12ff9-c7e1-4fc6-ad01-683bd90b3c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# smoothed and predicted log rates\n",
    "r_s = seq_vae.ssm.to(cfg.data_device).likelihood_pdf.readout_fn(z_s_test)\n",
    "r_p = seq_vae.ssm.to(cfg.data_device).likelihood_pdf.readout_fn(z_p_test)\n",
    "# sampeling from the rates possion to get the spike counts\n",
    "y_s = torch.poisson(cfg.bin_sz * torch.exp(r_s))\n",
    "y_p = torch.poisson(cfg.bin_sz * torch.exp(r_p))\n",
    "\n",
    "y_hat = torch.cat([y_s[:, :, :cfg.n_bins_bhv], y_p], dim=2)\n",
    "z_hat = torch.cat([z_s_test[:, :, :cfg.n_bins_bhv, :], z_p_test], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b808e-4acc-486e-a78e-59fd956ab689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rates_train_s = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s_train)).mean(dim=0)\n",
    "\n",
    "rates_test_s = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s_test)).mean(dim=0)\n",
    "rates_test_f = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_f_test)).mean(dim=0)\n",
    "rates_test_p = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_p_test)).mean(dim=0)\n",
    "\n",
    "rates_valid_s = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s_valid)).mean(dim=0)\n",
    "rates_valid_f = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_f_valid)).mean(dim=0)\n",
    "rates_valid_p = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_p_valid)).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081ceca-6941-4338-bcd7-046aadda08e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spikes_train_s = torch.poisson(rates_train_s)\n",
    "\n",
    "spikes_test_s = torch.poisson(rates_test_s)\n",
    "spikes_test_f = torch.poisson(rates_test_f)\n",
    "spikes_test_p = torch.poisson(rates_test_p)\n",
    "\n",
    "spikes_valid_s = torch.poisson(rates_valid_s)\n",
    "spikes_valid_f = torch.poisson(rates_valid_f)\n",
    "spikes_valid_p = torch.poisson(rates_valid_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb07c54-7a1d-4ba6-a413-983e8677be44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_spike_count = torch.sum(y_valid_obs, axis=1)\n",
    "model_spike_count = torch.sum(spikes_test_s, axis=1)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "fig.suptitle('spike counts')\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    axs[0].set_title('real')\n",
    "    dsc = axs[0].imshow(data_spike_count, interpolation='none', aspect='auto')\n",
    "    axs[1].set_title('generated')\n",
    "    msc = axs[1].imshow(model_spike_count, interpolation='none', aspect='auto')\n",
    "\n",
    "    vmin, _ = min(torch.min(data_spike_count.flatten(), dim=0), torch.min(model_spike_count.flatten(), dim=0))\n",
    "    vmax, _ = max(torch.max(data_spike_count.flatten(), dim=0), torch.max(model_spike_count.flatten(), dim=0))\n",
    "\n",
    "    fig.colorbar(dsc, ax=axs[0], shrink=0.4).mappable.set_clim(vmin=vmin, vmax=vmax)\n",
    "    fig.colorbar(msc, ax=axs[1], shrink=0.4).mappable.set_clim(vmin=vmin, vmax=vmax)\n",
    "                \n",
    "axs[1].set_xlabel('neuron')\n",
    "axs[0].set_ylabel('trial')\n",
    "\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f24cf9-8484-4f81-ae45-c47a79605c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    trial_idx = np.random.randint(0, y_test_obs.shape[0])\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(4, 6))\n",
    "    plot_spikes(y_test_obs[trial_idx], axs[0])\n",
    "    plot_spikes(spikes_test_s[trial_idx], axs[1]) # from the first posterior sample (dim0 = 0)\n",
    "\n",
    "    fig.suptitle(f'trial {trial_idx}\\n(active neurons ordered by their spike counts)', fontsize=10)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41651780-26c3-4d18-b601-e56bb9d6764a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    n_eurons_to_plot = 16\n",
    "    neuron_incs = np.random.choice(range(0, y_valid_obs.shape[2]), size=n_eurons_to_plot, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(int(np.sqrt(n_eurons_to_plot)), int(np.sqrt(n_eurons_to_plot)), figsize=(9, 7))\n",
    "    fig.suptitle(f'PSTH\\n\\n\\n')\n",
    "\n",
    "    for ax, neuron in zip(axes.flat, neuron_incs):\n",
    "\n",
    "        fr_data = torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(y_valid_obs[:, :, neuron], axis=0) / cfg.bin_sz,\n",
    "                sigma=2.5, axis=0))\n",
    "\n",
    "        fr_model= torch.tensor(\n",
    "            gaussian_filter1d(\n",
    "                torch.mean(rates_test_s, axis=0)[:, neuron] / cfg.bin_sz,\n",
    "                sigma=2.5, axis=0))\n",
    "\n",
    "        ax.plot(np.arange(n_time_bins) * cfg.bin_sz_ms, fr_data, color= 'black', alpha=0.8, label='data' if neuron == neuron_incs[-1] else '')\n",
    "        ax.plot(np.arange(n_time_bins) * cfg.bin_sz_ms, fr_model, color= 'red', alpha=0.4, label='model' if neuron == neuron_incs[-1] else '')\n",
    "\n",
    "        ax.set_title(f'neuron {neuron+1}', fontsize=8)\n",
    "        ax.set_xlabel('time (sec))' if neuron == neuron_incs[-int(np.sqrt(n_eurons_to_plot))] else '', fontsize=9)\n",
    "        ax.set_ylabel('firing rate' if neuron == neuron_incs[0] else '', fontsize=9)\n",
    "        ax.tick_params(axis='x', labelsize=8)\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "        ax.axvline(x=cfg.move_onset*cfg.bin_sz_ms, color='y', linestyle='--')\n",
    "        \n",
    "        if neuron == neuron_incs[0]:\n",
    "            y_upper_limit = ax.get_ylim()[1]\n",
    "\n",
    "            ax.annotate('move\\nonset',\n",
    "                        xy=(cfg.move_onset*cfg.bin_sz_ms, y_upper_limit),\n",
    "                        xytext=((cfg.move_onset*cfg.bin_sz_ms)*0.4, y_upper_limit*1.2),\n",
    "                        arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                        fontsize=7, ha='center')\n",
    "        \n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.94), ncol=1, fontsize=8)\n",
    "    fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f3cfd-b70c-4bfe-b17f-d1d7a91c807a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials_inds = np.random.choice(range(0, y_valid_obs.shape[0]), size=4, replace=False)\n",
    "\n",
    "plot_rastor(y_valid_obs, z_s_test, trials_inds, y_valid_obs.shape[2], cfg)\n",
    "plot_rastor(spikes_test_s, z_s_test, trials_inds, spikes_test_s.shape[2], cfg, regime='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f7527-2974-4db8-be46-27ec327996fa",
   "metadata": {},
   "source": [
    "## Ordering the neurons in terms of their loading to the 1st principal latent dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91935f70-c816-4ee1-b96d-1e27e584a409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_idx = np.random.randint(0, y_valid_obs.shape[0])\n",
    "ordered_correlations, ordered_neurons = order_neurons(y_valid_obs, z_s_test, trial=trial_idx, latent=0)\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "time_bins = [str(i) for i in ordered_neurons[:top_n].astype('int32')]\n",
    "\n",
    "y = np.abs(ordered_correlations.T[1][:top_n])\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "axes.plot(time_bins, y, marker='o', linestyle='-', color='g')\n",
    "\n",
    "for i in range(len(time_bins)):\n",
    "    plt.text(time_bins[i], y[i]+0.005, f'{y[i]:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    plt.vlines(i, ymin=y.min(), ymax=y[i], linestyles='dashed', colors='gray', alpha=0.5)\n",
    "\n",
    "fig.suptitle(f'top 10 neurons in trial {trial_idx+1} ordered based on their loading\\nto the 1st principal latent dimension\\n\\n\\n\\n')\n",
    "plt.ylim(top=y.max()+0.05*y.max())\n",
    "\n",
    "plt.xlabel('top 10 neurons')\n",
    "plt.ylabel('correlation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599b55d-24c7-4795-994b-26f9965252b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials_inds = np.random.choice(range(0, y_valid_obs.shape[0]), size=4, replace=False)\n",
    "\n",
    "plot_rastor(y_valid_obs, z_s_test, trials_inds, y_valid_obs.shape[2], cfg, order=True)\n",
    "plot_rastor(spikes_test_s, z_s_test, trials_inds, spikes_test_s.shape[2], cfg, regime='prediction', order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca70fb-899f-4798-a940-b24a020b42b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf079c80-1c38-4b39-9917-909dc972fa19",
   "metadata": {},
   "source": [
    "# Velocity decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7485b0-b5bc-4698-8bc9-fb050f09e01e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"\", job_name=\"monkey_reaching\")\n",
    "cfg = compose(config_name=\"config\")\n",
    "cfg.data_device = 'cpu'\n",
    "cfg.device = 'cpu'\n",
    "\n",
    "lightning.seed_everything(cfg.seed, workers=True)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3dcb5-46ef-4ff2-9c9c-d2bf4ba39b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Due to the structure of the mc_maze dataset, for training the velocity decoder we have to reperform the inference (on y_train _valid and _test)\n",
    "but taking the first 35 time bins, because the velocity tensors also have only the first 35 time bins.\n",
    "'''\n",
    "data_path = 'data/data_{split}_{bin_sz_ms}ms.pt'\n",
    "train_data = torch.load(data_path.format(split='train', bin_sz_ms=cfg.bin_sz_ms))\n",
    "val_data = torch.load(data_path.format(split='test', bin_sz_ms=cfg.bin_sz_ms))\n",
    "test_data = torch.load(data_path.format(split='test', bin_sz_ms=cfg.bin_sz_ms))\n",
    "\n",
    "y_valid_obs = val_data['y_obs'].type(torch.float32).to(cfg.data_device)[:, :35]\n",
    "y_train_obs = train_data['y_obs'].type(torch.float32).to(cfg.data_device)[:, :35]\n",
    "y_test_obs = test_data['y_obs'].type(torch.float32).to(cfg.data_device)[:, :35]\n",
    "vel_valid = val_data['velocity'].type(torch.float32).to(cfg.data_device)\n",
    "vel_train = train_data['velocity'].type(torch.float32).to(cfg.data_device)\n",
    "vel_test = test_data['velocity'].type(torch.float32).to(cfg.data_device)\n",
    "n_trials, n_time_bins, n_neurons_obs = y_train_obs.shape\n",
    "n_time_bins_enc = train_data['n_time_bins_enc']\n",
    "batch_sz_test = list(y_test_obs.shape)[:-1]\n",
    "n_trials_test = y_test_obs.shape[0]\n",
    "\n",
    "y_train_dataset = torch.utils.data.TensorDataset(y_train_obs, vel_train)\n",
    "y_val_dataset = torch.utils.data.TensorDataset(y_valid_obs, vel_valid)\n",
    "y_test_dataset = torch.utils.data.TensorDataset(y_test_obs, vel_test)\n",
    "train_dataloader = torch.utils.data.DataLoader(y_train_dataset, batch_size=cfg.batch_sz, shuffle=False)\n",
    "valid_dataloader = torch.utils.data.DataLoader(y_val_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(y_test_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)\n",
    "\n",
    "print(\"\\033[1m\\nNow the data are in {0} time bins, first {1} bins from the inferred latents are used to predict the remaining {2} time steps of the trajectory in the prediction regime.\\n\\033[0m\"\n",
    "      .format(n_time_bins, cfg.n_bins_bhv, n_time_bins - cfg.n_bins_bhv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8aa125-e53c-491a-9a4a-4b78e72f358a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"likelihood pdf\"\"\"\n",
    "H = utils.ReadoutLatentMask(cfg.n_latents, cfg.n_latents_read)\n",
    "readout_fn = nn.Sequential(H, nn.Linear(cfg.n_latents_read, n_neurons_obs))\n",
    "readout_fn[-1].bias.data = prob_utils.estimate_poisson_rate_bias(train_dataloader, cfg.bin_sz)\n",
    "likelihood_pdf = PoissonLikelihood(readout_fn, n_neurons_obs, cfg.bin_sz, device=cfg.device)\n",
    "\n",
    "\"\"\"dynamics module\"\"\"\n",
    "Q_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "dynamics_fn = utils.build_gru_dynamics_function(cfg.n_latents, cfg.n_hidden_dynamics, device=cfg.device)\n",
    "dynamics_mod = DenseGaussianDynamics(dynamics_fn, cfg.n_latents, Q_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"initial condition\"\"\"\n",
    "m_0 = torch.zeros(cfg.n_latents, device=cfg.device)\n",
    "Q_0_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "initial_condition_pdf = DenseGaussianInitialCondition(cfg.n_latents, m_0, Q_0_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"local/backward encoder\"\"\"\n",
    "backward_encoder = BackwardEncoderLRMvn(cfg.n_latents, cfg.n_hidden_backward, cfg.n_latents,\n",
    "                                        rank_local=cfg.rank_local, rank_backward=cfg.rank_backward,\n",
    "                                        device=cfg.device)\n",
    "local_encoder = LocalEncoderLRMvn(cfg.n_latents, n_neurons_obs, cfg.n_hidden_local, cfg.n_latents,\n",
    "                                  rank=cfg.rank_local,\n",
    "                                  device=cfg.device, dropout=cfg.p_local_dropout)\n",
    "nl_filter = NonlinearFilter(dynamics_mod, initial_condition_pdf, device=cfg.device)\n",
    "\n",
    "\"\"\"sequence vae\"\"\"\n",
    "ssm = LowRankNonlinearStateSpaceModel(dynamics_mod, likelihood_pdf, initial_condition_pdf, backward_encoder,\n",
    "                                      local_encoder, nl_filter, device=cfg.device)\n",
    "\n",
    "\"\"\"lightning\"\"\"\n",
    "best_model_path = 'ckpts/smoother/acausal/epoch=692_valid_loss=1415.51_r2_valid_enc=0.89_r2_valid_bhv=0.73_valid_bps_enc=0.41.ckpt'\n",
    "seq_vae = LightningMonkeyReaching.load_from_checkpoint(best_model_path, ssm=ssm, cfg=cfg,\n",
    "                                                       n_time_bins_enc=n_time_bins_enc, n_time_bins_bhv=cfg.n_bins_bhv,\n",
    "                                                       strict=False)\n",
    "seq_vae.ssm = seq_vae.ssm.to(cfg.device)\n",
    "seq_vae.ssm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44c384-118e-41ab-ac56-7700e5092c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"re-performing inference with new # of bins; so that the data matches the targets\"\"\"\n",
    "z_s_train = []\n",
    "\n",
    "z_s_test = []\n",
    "z_f_test = []\n",
    "z_p_test = []\n",
    "\n",
    "z_s_valid = []\n",
    "z_f_valid = []\n",
    "z_p_valid = []\n",
    "\n",
    "m_0 = seq_vae.ssm.nl_filter.initial_c_pdf.m_0\n",
    "Q_0 = Fn.softplus(seq_vae.ssm.nl_filter.initial_c_pdf.log_Q_0)\n",
    "#m_0 = seq_vae.ssm.dynamics_mod.initial_c_pdf.m_0\n",
    "#Q_0 = Fn.softplus(seq_vae.ssm.dynamics_mod.initial_c_pdf.log_Q_0)\n",
    "\n",
    "z_ic = m_0 + Q_0.sqrt() * torch.randn(batch_sz_test + [cfg.n_latents], device=cfg.device)\n",
    "z_ic_p = seq_vae.ssm.predict_forward(z_ic, 35 - cfg.n_bins_bhv)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    loss, z, stats = seq_vae.ssm(batch[0], cfg.n_samples)\n",
    "    z_s_train.append(z)\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    z_f, stats = seq_vae.ssm.fast_filter_1_to_T(batch[0], cfg.n_samples)\n",
    "    loss, z, stats = seq_vae.ssm(batch[0], cfg.n_samples)\n",
    "    z_p = seq_vae.ssm.predict_forward(z_f[:, :, cfg.n_bins_bhv], 35-cfg.n_bins_bhv)\n",
    "    z_p = torch.cat([z_f[:, :, :cfg.n_bins_bhv], z_p], dim=2)\n",
    "    z_f_test.append(z_f)\n",
    "    z_p_test.append(z_p)\n",
    "    z_s_test.append(z)\n",
    "    \n",
    "for batch in valid_dataloader:\n",
    "    z_f, stats = seq_vae.ssm.fast_filter_1_to_T(batch[0], cfg.n_samples)\n",
    "    loss, z, stats = seq_vae.ssm(batch[0], cfg.n_samples)\n",
    "    z_p = seq_vae.ssm.predict_forward(z_f[:, :, cfg.n_bins_bhv], cfg.n_samples)\n",
    "    z_p = torch.cat([z_f[:, :, :10], z_p], dim=2)\n",
    "    z_f_valid.append(z_f)\n",
    "    z_p_valid.append(z_p)\n",
    "    z_s_valid.append(z)\n",
    "\n",
    "z_s_train = torch.cat(z_s_train, dim=1)\n",
    "\n",
    "z_s_test = torch.cat(z_s_test, dim=1)\n",
    "z_f_test = torch.cat(z_f_test, dim=1)\n",
    "z_p_test = torch.cat(z_p_test, dim=1)\n",
    "\n",
    "z_s_valid = torch.cat(z_s_valid, dim=1)\n",
    "z_f_valid = torch.cat(z_f_valid, dim=1)\n",
    "z_p_valid = torch.cat(z_p_valid, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9de78d-4691-4721-b8a3-02d4cbe678f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rates_train_s = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s_train)).mean(dim=0)\n",
    "\n",
    "rates_test_s = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s_test)).mean(dim=0)\n",
    "rates_test_f = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_f_test)).mean(dim=0)\n",
    "rates_test_p = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_p_test)).mean(dim=0)\n",
    "\n",
    "rates_valid_s = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_s_valid)).mean(dim=0)\n",
    "rates_valid_f = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_f_valid)).mean(dim=0)\n",
    "rates_valid_p = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_p_valid)).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee04e8d-8060-42bd-abfe-859ac4da81a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Note that the decoder is trained on the rates generated by XFADS from the latents inferred from the training data.\n",
    "But tested on the rates generated from the latents inferred from the validation and the testing data.\n",
    "'''\n",
    "with torch.no_grad():\n",
    "    clf = Ridge(alpha=0.01)\n",
    "    # fit to training data\n",
    "    clf.fit(rates_train_s.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    r2 = clf.score(rates_train_s.reshape(-1, n_neurons_obs), vel_train.reshape(-1, 2))\n",
    "    r2_test = clf.score(rates_test_s.reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2))\n",
    "    r2_filter = clf.score(rates_test_f.reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2))\n",
    "    r2_k_step = []\n",
    "\n",
    "    # transform test data\n",
    "    r2_test_s = clf.score(rates_test_s.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    r2_test_f = clf.score(rates_test_f.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    r2_test_p = clf.score(rates_test_p.reshape(-1, n_neurons_obs), vel_test.reshape(-1, 2))\n",
    "    vel_hat_test_s = clf.predict(rates_test_s.reshape(-1, n_neurons_obs)).reshape(list(batch_sz_test) + [2])\n",
    "    vel_hat_test_f = clf.predict(rates_test_f.reshape(-1, n_neurons_obs)).reshape(list(batch_sz_test) + [2])\n",
    "    vel_hat_test_p = clf.predict(rates_test_p.reshape(-1, n_neurons_obs)).reshape(list(batch_sz_test) + [2])\n",
    "    \n",
    "    for k in range(30):\n",
    "        z_prd_test = utils.propagate_latent_k_steps(z_f_valid[:, :, k], dynamics_mod, n_time_bins + 0 - (k + 1))\n",
    "        z_prd_test = torch.concat([z_f_valid[:, :, :k], z_prd_test], dim=2)\n",
    "\n",
    "        # m_prd_test = z_prd_test.mean(dim=0)\n",
    "        # m_prd_test = torch.concat([m_filter[:, :k], m_prd_test[:, k:]], dim=1)\n",
    "\n",
    "        rates_prd_test = cfg.bin_sz * torch.exp(seq_vae.ssm.likelihood_pdf.readout_fn(z_prd_test)).mean(dim=0)\n",
    "\n",
    "        r2_prd = clf.score(rates_prd_test.reshape(-1, n_neurons_obs), vel_valid.reshape(-1, 2))\n",
    "        r2_k_step.append(r2_prd)\n",
    "\n",
    "    vel_to_pos = lambda v: torch.cumsum(torch.tensor(v), dim=1)\n",
    "\n",
    "    pos_test = vel_to_pos(vel_test)\n",
    "    pos_test_hat_s = vel_to_pos(vel_hat_test_s)\n",
    "    pos_test_hat_f = vel_to_pos(vel_hat_test_f)\n",
    "    pos_test_hat_p = vel_to_pos(vel_hat_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ab78b-a141-4a4a-b4ba-021ce81ca84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"plotting\"\"\"\n",
    "blues = cm.get_cmap(\"Blues\", n_samples_mu_plt)\n",
    "grays = cm.get_cmap(\"Greys\", n_samples_mu_plt)\n",
    "yellows = cm.get_cmap(\"YlOrBr\", n_samples_mu_plt)\n",
    "\n",
    "n_trials_plot = 125\n",
    "n_samples_mu_plt = 20\n",
    "\n",
    "trial_plt_dx = torch.randperm(n_trials_test)[:n_trials_plot]\n",
    "reach_angle = torch.atan2(pos_test[:, -1, 0], pos_test[:, -1, 1])\n",
    "reach_colors = plt.cm.hsv(reach_angle / (2 * np.pi) + 0.5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    plot_utils.plot_reaching(axs[0], pos_test[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[1], pos_test_hat_s[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[2], pos_test_hat_f[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "    plot_utils.plot_reaching(axs[3], pos_test_hat_p[trial_plt_dx], reach_colors[trial_plt_dx])\n",
    "\n",
    "    axs[0].set_title('true')\n",
    "    axs[1].set_title(f'smoothed, r2:{r2_test_s:.3f}')\n",
    "    axs[2].set_title(f'filtered, r2:{r2_test_f:.3f}')\n",
    "    axs[3].set_title(f'predicted, r2:{r2_test_p:.3f}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f7bf2-eb77-4d4c-967e-0e12b2682c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.axvline(12, linestyle='--')\n",
    "plt.plot(r2_k_step)\n",
    "plt.axhline(r2_test, color='green', label='smoothed')\n",
    "plt.axhline(r2_filter, color='orange', label='filtered')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aa145-d977-4115-a662-16421da6f4d9",
   "metadata": {},
   "source": [
    "## Evaluating model performance with different time intervals prior to movement to estimate the latents\n",
    "This animation compares how well the unrolled latent state can predict behavior if only data up until t ms (denoted in the previous figures as: `movement_onset - prediction_starts`) before the movement is used to causally construct estimates of the latent state up until the time its unrolled.\n",
    "\n",
    "You can try different data windows by changing the point in time where the estimation of the latents starts, `cfg.n_bins_bhv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b672ec-cb58-488c-b7dd-e05f28d4c804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video('animation.mp4', embed=True, width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c240f-ef78-42c7-86f1-0523315cc87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video('predicted_trajectories_animation.mp4', embed=True, width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd7f0e-7c96-418c-bc06-b170afb77b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video('s_vs_p_trajectories_animation.mp4', embed=True, width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47414347-713e-4e57-842b-7b169bb21b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfads",
   "language": "python",
   "name": "xfads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
