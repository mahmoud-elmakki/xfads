{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9a3bcab-9855-4d45-bb5f-49709b17829b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"  # export OMP_NUM_THREADS=4\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"  # export OPENBLAS_NUM_THREADS=4\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"  # export MKL_NUM_THREADS=6\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\"  # export VECLIB_MAXIMUM_THREADS=4\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"  # export NUMEXPR_NUM_THREADS=6\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xfads.utils as utils\n",
    "import xfads.prob_utils as prob_utils\n",
    "import pytorch_lightning as lightning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from xfads.ssm_modules.likelihoods import PoissonLikelihood\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianDynamics\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianInitialCondition\n",
    "from xfads.ssm_modules.encoders import LocalEncoderLRMvn, BackwardEncoderLRMvn\n",
    "from xfads.smoothers.lightning_trainers import LightningNonlinearSSM, LightningDMFCRSG\n",
    "from xfads.smoothers.nonlinear_smoother import NonlinearFilter, LowRankNonlinearStateSpaceModel\n",
    "# from dev.smoothers.nonlinear_smoother_causal_debug import NonlinearFilter, LowRankNonlinearStateSpaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8611875a-ed0f-40a0-9ddb-d876cbb69eab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "initialize(version_base=None, config_path=\"\", job_name=\"dmfc_rsg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "06b0cbdd-15b2-40cf-9c9a-d32b5ec657c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f015e6ac-6477-44ec-86e7-f0b309b07fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seeds = [1234, 1235, 1236]\n",
    "# seeds = [1235, 1236]\n",
    "seed = 1239\n",
    "n_bins_bhv = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b26a57bc-2762-475d-8bee-ad14c8d4d44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1239\n"
     ]
    }
   ],
   "source": [
    "\"\"\"config\"\"\"\n",
    "\n",
    "cfg.seed = seed\n",
    "\n",
    "lightning.seed_everything(cfg.seed, workers=True)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d8c9d303-0f9b-4cd5-8a11-8b526d87220f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.7.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.7.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "MultiIndex([(    'align_time',   ''),\n",
      "            (    'clock_time',   ''),\n",
      "            ('heldout_spikes', 1007),\n",
      "            ('heldout_spikes', 1016),\n",
      "            ('heldout_spikes', 1017),\n",
      "            ('heldout_spikes', 1053),\n",
      "            ('heldout_spikes', 1054),\n",
      "            ('heldout_spikes', 1063),\n",
      "            ('heldout_spikes', 1071),\n",
      "            ('heldout_spikes', 1099),\n",
      "            ('heldout_spikes', 1101),\n",
      "            ('heldout_spikes', 1103),\n",
      "            ('heldout_spikes', 1108),\n",
      "            ('heldout_spikes', 2098),\n",
      "            ('heldout_spikes', 3052),\n",
      "            ('heldout_spikes', 3103),\n",
      "            (        'margin',   ''),\n",
      "            (        'spikes', 1001),\n",
      "            (        'spikes', 1002),\n",
      "            (        'spikes', 1003),\n",
      "            (        'spikes', 1004),\n",
      "            (        'spikes', 1012),\n",
      "            (        'spikes', 1019),\n",
      "            (        'spikes', 1024),\n",
      "            (        'spikes', 1026),\n",
      "            (        'spikes', 1028),\n",
      "            (        'spikes', 1029),\n",
      "            (        'spikes', 1030),\n",
      "            (        'spikes', 1038),\n",
      "            (        'spikes', 1042),\n",
      "            (        'spikes', 1044),\n",
      "            (        'spikes', 1048),\n",
      "            (        'spikes', 1049),\n",
      "            (        'spikes', 1074),\n",
      "            (        'spikes', 1088),\n",
      "            (        'spikes', 1093),\n",
      "            (        'spikes', 1094),\n",
      "            (        'spikes', 1102),\n",
      "            (        'spikes', 1104),\n",
      "            (        'spikes', 1105),\n",
      "            (        'spikes', 1106),\n",
      "            (        'spikes', 1111),\n",
      "            (        'spikes', 1112),\n",
      "            (        'spikes', 1114),\n",
      "            (        'spikes', 1115),\n",
      "            (        'spikes', 1121),\n",
      "            (        'spikes', 2059),\n",
      "            (        'spikes', 3001),\n",
      "            (        'spikes', 3007),\n",
      "            (        'spikes', 3008),\n",
      "            (        'spikes', 3036),\n",
      "            (        'spikes', 3049),\n",
      "            (        'spikes', 3054),\n",
      "            (        'spikes', 3086),\n",
      "            (        'spikes', 3097),\n",
      "            (        'spikes', 3100),\n",
      "            (        'spikes', 3101),\n",
      "            (      'trial_id',   ''),\n",
      "            (    'trial_time',   '')],\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahmoud/catnip/xfads/workshop/monkey_timing/download_data.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tp.append(torch.tensor(tp_t).unsqueeze(-1))\n",
      "/Users/mahmoud/catnip/xfads/workshop/monkey_timing/download_data.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ts.append(torch.tensor(ts_t).unsqueeze(-1))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"downloading the data\"\"\"\n",
    "\n",
    "%run download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bb00e154-f8ed-4401-b27c-6abb9f83bb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"loading the data\"\"\"\n",
    "\n",
    "data_path = 'data/data_{split}_{bin_sz_ms}ms.pt'\n",
    "train_data = torch.load(data_path.format(split='train', bin_sz_ms=cfg.bin_sz_ms))\n",
    "val_data = torch.load(data_path.format(split='valid', bin_sz_ms=cfg.bin_sz_ms))\n",
    "test_data = torch.load(data_path.format(split='test', bin_sz_ms=cfg.bin_sz_ms))\n",
    "\n",
    "y_valid_obs = val_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_train_obs = train_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_test_obs = test_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "n_trials, n_time_bins, n_neurons_obs = y_train_obs.shape\n",
    "n_time_bins_enc = train_data['n_time_bins_enc']\n",
    "\n",
    "y_train_dataset = torch.utils.data.TensorDataset(y_train_obs, )\n",
    "y_val_dataset = torch.utils.data.TensorDataset(y_valid_obs, )\n",
    "y_test_dataset = torch.utils.data.TensorDataset(y_test_obs, )\n",
    "train_dataloader = torch.utils.data.DataLoader(y_train_dataset, batch_size=cfg.batch_sz, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(y_val_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(y_test_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be7e4047-83d8-4beb-988a-5d562d93858c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 260, 54])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6d7d8ec3-07b9-4bc6-af91-0b4889d6ba2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "\n",
    "from itertools import product\n",
    "from nlb_tools.nwb_interface import NWBDataset\n",
    "\n",
    "\n",
    "def get_int_to_verbose_map():\n",
    "    options = [('S', 'L'), ('E', 'H'), ('1', '2', '3', '4', '5')]\n",
    "    combinations = list(product(*options))\n",
    "    mapping = {i: combination for i, combination in enumerate(combinations)}\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def get_verbose_to_int_map():\n",
    "    options = [('S', 'L'), ('E', 'H'), ('1', '2', '3', '4', '5')]\n",
    "    combinations = list(product(*options))\n",
    "    reverse_mapping = {combination: i for i, combination in enumerate(combinations)}\n",
    "\n",
    "    return reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b0bc5563-0127-45c6-9d05-8bb1e35707dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.7.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.8.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'core' version 2.4.0 because version 2.7.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/hdmf/utils.py:668: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.5.0 is already loaded.\n",
      "  return func(args[0], **pargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "MultiIndex([(    'align_time',   ''),\n",
      "            (    'clock_time',   ''),\n",
      "            ('heldout_spikes', 1007),\n",
      "            ('heldout_spikes', 1016),\n",
      "            ('heldout_spikes', 1017),\n",
      "            ('heldout_spikes', 1053),\n",
      "            ('heldout_spikes', 1054),\n",
      "            ('heldout_spikes', 1063),\n",
      "            ('heldout_spikes', 1071),\n",
      "            ('heldout_spikes', 1099),\n",
      "            ('heldout_spikes', 1101),\n",
      "            ('heldout_spikes', 1103),\n",
      "            ('heldout_spikes', 1108),\n",
      "            ('heldout_spikes', 2098),\n",
      "            ('heldout_spikes', 3052),\n",
      "            ('heldout_spikes', 3103),\n",
      "            (        'margin',   ''),\n",
      "            (        'spikes', 1001),\n",
      "            (        'spikes', 1002),\n",
      "            (        'spikes', 1003),\n",
      "            (        'spikes', 1004),\n",
      "            (        'spikes', 1012),\n",
      "            (        'spikes', 1019),\n",
      "            (        'spikes', 1024),\n",
      "            (        'spikes', 1026),\n",
      "            (        'spikes', 1028),\n",
      "            (        'spikes', 1029),\n",
      "            (        'spikes', 1030),\n",
      "            (        'spikes', 1038),\n",
      "            (        'spikes', 1042),\n",
      "            (        'spikes', 1044),\n",
      "            (        'spikes', 1048),\n",
      "            (        'spikes', 1049),\n",
      "            (        'spikes', 1074),\n",
      "            (        'spikes', 1088),\n",
      "            (        'spikes', 1093),\n",
      "            (        'spikes', 1094),\n",
      "            (        'spikes', 1102),\n",
      "            (        'spikes', 1104),\n",
      "            (        'spikes', 1105),\n",
      "            (        'spikes', 1106),\n",
      "            (        'spikes', 1111),\n",
      "            (        'spikes', 1112),\n",
      "            (        'spikes', 1114),\n",
      "            (        'spikes', 1115),\n",
      "            (        'spikes', 1121),\n",
      "            (        'spikes', 2059),\n",
      "            (        'spikes', 3001),\n",
      "            (        'spikes', 3007),\n",
      "            (        'spikes', 3008),\n",
      "            (        'spikes', 3036),\n",
      "            (        'spikes', 3049),\n",
      "            (        'spikes', 3054),\n",
      "            (        'spikes', 3086),\n",
      "            (        'spikes', 3097),\n",
      "            (        'spikes', 3100),\n",
      "            (        'spikes', 3101),\n",
      "            (      'trial_id',   ''),\n",
      "            (    'trial_time',   '')],\n",
      "           )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/p6knbk5d2qzdvstddyrnfym80000gn/T/ipykernel_63528/4245514269.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tp.append(torch.tensor(tp_t).unsqueeze(-1))\n",
      "/var/folders/py/p6knbk5d2qzdvstddyrnfym80000gn/T/ipykernel_63528/4245514269.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ts.append(torch.tensor(ts_t).unsqueeze(-1))\n"
     ]
    }
   ],
   "source": [
    "datapath = 'data/000130/sub-Haydn'\n",
    "dataset = NWBDataset(datapath)\n",
    "save_root_path = 'data/'\n",
    "\n",
    "# Extract neural data and lagged hand velocity\n",
    "binsize = 10\n",
    "n_neurons = 54\n",
    "dataset.resample(binsize)\n",
    "\n",
    "start = -1300\n",
    "end = 1300\n",
    "trial_length = (end - start) // binsize\n",
    "\n",
    "verbose_to_int_map = get_verbose_to_int_map()\n",
    "int_to_verbose_map = get_int_to_verbose_map()\n",
    "\n",
    "# Extract neural data\n",
    "trial_info = dataset.trial_info  # .dropna()\n",
    "trial_info['color'] = None\n",
    "trial_info['position_id'] = None\n",
    "trial_data = dataset.make_trial_data(align_field='set_time', align_range=(start, end))\n",
    "n_trials = trial_data.shape[0] // trial_length\n",
    "\n",
    "y = []\n",
    "tp = []\n",
    "ts = []\n",
    "task_id = []\n",
    "\n",
    "print('done')\n",
    "print(trial_data.columns)\n",
    "count = 0\n",
    "for trial_id, trial in trial_data.groupby('trial_id'):\n",
    "    trial_id_trial_info = trial_info[trial_info['trial_id'] == trial_id]\n",
    "    is_outlier_t = trial_id_trial_info['is_outlier'].iloc[0]\n",
    "    tp_t = torch.tensor(trial_id_trial_info['tp'].iloc[0])\n",
    "    ts_t = torch.tensor(trial_id_trial_info['ts'].iloc[0])\n",
    "    is_short_t = trial_id_trial_info['is_short'].iloc[0]\n",
    "    is_eye_t = trial_id_trial_info['is_eye'].iloc[0]\n",
    "\n",
    "    if is_outlier_t or tp_t < 0:\n",
    "        continue\n",
    "\n",
    "    if is_short_t:\n",
    "        task_str_1 = 'S'\n",
    "\n",
    "        if ts_t == 480:\n",
    "            task_str_3 = '1'\n",
    "        elif ts_t == 560:\n",
    "            task_str_3 = '2'\n",
    "        elif ts_t == 640:\n",
    "            task_str_3 = '3'\n",
    "        elif ts_t == 720:\n",
    "            task_str_3 = '4'\n",
    "        elif ts_t == 800:\n",
    "            task_str_3 = '5'\n",
    "    else:\n",
    "        task_str_1 = 'L'\n",
    "\n",
    "        if ts_t == 800:\n",
    "            task_str_3 = '1'\n",
    "        elif ts_t == 900:\n",
    "            task_str_3 = '2'\n",
    "        elif ts_t == 1000:\n",
    "            task_str_3 = '3'\n",
    "        elif ts_t == 1100:\n",
    "            task_str_3 = '4'\n",
    "        elif ts_t == 1200:\n",
    "            task_str_3 = '5'\n",
    "\n",
    "    if is_eye_t:\n",
    "        task_str_2 = 'E'\n",
    "    else:\n",
    "        task_str_2 = 'H'\n",
    "\n",
    "    y_heldin_t = torch.tensor(trial.spikes.values)\n",
    "    y_heldout_t = torch.tensor(trial.heldout_spikes.values)\n",
    "    y_t = torch.concat([y_heldin_t, y_heldout_t], dim=-1)\n",
    "    y.append(y_t.reshape(1, trial_length, n_neurons))\n",
    "\n",
    "    task_id_key = (task_str_1, task_str_2, task_str_3)\n",
    "    task_id_int = verbose_to_int_map[task_id_key]\n",
    "    task_id.append(torch.tensor(task_id_int).unsqueeze(-1))\n",
    "\n",
    "    tp.append(torch.tensor(tp_t).unsqueeze(-1))\n",
    "    ts.append(torch.tensor(ts_t).unsqueeze(-1))\n",
    "\n",
    "    if is_outlier_t:\n",
    "        count += 1\n",
    "\n",
    "y = torch.concat(y, dim=0)\n",
    "task_id = torch.concat(task_id, dim=0)\n",
    "\n",
    "subset_ex = 10\n",
    "subset_ex_loc = torch.where(task_id == subset_ex)[0]\n",
    "\n",
    "y_subset = y[subset_ex_loc]\n",
    "y_psth = y_subset.mean(dim=0)\n",
    "\n",
    "ts = torch.stack(ts, dim=0)\n",
    "tp = torch.stack(tp, dim=0)\n",
    "\n",
    "with open('data/old_data/int_condition_map.yaml', 'w') as outfile:\n",
    "    yaml.dump(int_to_verbose_map, outfile, default_flow_style=False)\n",
    "\n",
    "train_data, valid_data, test_data = {}, {}, {}\n",
    "untrained_trials = 300\n",
    "seq_len = trial_length\n",
    "\n",
    "train_data['y_obs'] = y[:-untrained_trials]\n",
    "train_data['task_id'] = task_id[:-untrained_trials]\n",
    "train_data['ts'] = ts[:-untrained_trials]\n",
    "train_data['tp'] = tp[:-untrained_trials]\n",
    "train_data['n_neurons_enc'] = n_neurons\n",
    "train_data['n_neurons_obs'] = n_neurons\n",
    "train_data['n_time_bins_enc'] = seq_len\n",
    "\n",
    "valid_data['y_obs'] = y[-untrained_trials:-untrained_trials // 2]\n",
    "valid_data['task_id'] = task_id[-untrained_trials:-untrained_trials // 2]\n",
    "valid_data['ts'] = ts[-untrained_trials:-untrained_trials // 2]\n",
    "valid_data['tp'] = tp[-untrained_trials:-untrained_trials // 2]\n",
    "valid_data['n_neurons_enc'] = n_neurons\n",
    "valid_data['n_neurons_obs'] = n_neurons\n",
    "valid_data['n_time_bins_enc'] = seq_len\n",
    "\n",
    "test_data['y_obs'] = y[-untrained_trials // 2:]\n",
    "test_data['task_id'] = task_id[-untrained_trials // 2:]\n",
    "test_data['ts'] = ts[-untrained_trials // 2:]\n",
    "test_data['tp'] = tp[-untrained_trials // 2:]\n",
    "test_data['n_neurons_enc'] = n_neurons\n",
    "test_data['n_neurons_obs'] = n_neurons\n",
    "test_data['n_time_bins_enc'] = seq_len\n",
    "\n",
    "torch.save(train_data, save_root_path + f'data_train_{binsize}ms.pt')\n",
    "torch.save(valid_data, save_root_path + f'data_valid_{binsize}ms.pt')\n",
    "torch.save(test_data, save_root_path + f'data_test_{binsize}ms.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "68889853-d738-4d9d-9a75-c6b1556b0628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/IPython/core/formatters.py:344: FutureWarning: Index.format is deprecated and will be removed in a future version. Convert using index.astype(str) or index.map(formatter) instead.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>go_time</th>\n",
       "      <th>split</th>\n",
       "      <th>fix_on_time</th>\n",
       "      <th>fix_time</th>\n",
       "      <th>target_on_time</th>\n",
       "      <th>ready_time</th>\n",
       "      <th>set_time</th>\n",
       "      <th>...</th>\n",
       "      <th>theta</th>\n",
       "      <th>ts</th>\n",
       "      <th>tp</th>\n",
       "      <th>fix_time_dur</th>\n",
       "      <th>target_time_dur</th>\n",
       "      <th>iti</th>\n",
       "      <th>reward_dur</th>\n",
       "      <th>is_outlier</th>\n",
       "      <th>color</th>\n",
       "      <th>position_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>0 days 00:00:01.700000</td>\n",
       "      <td>0 days 00:00:01.500000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0 days 00:00:01.800000</td>\n",
       "      <td>0 days 00:00:03.500000</td>\n",
       "      <td>0 days 00:00:03.300000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0 days 00:00:03.600000</td>\n",
       "      <td>0 days 00:00:05.300000</td>\n",
       "      <td>0 days 00:00:05.100000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0 days 00:00:05.400000</td>\n",
       "      <td>0 days 00:00:07.100000</td>\n",
       "      <td>0 days 00:00:06.900000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0 days 00:00:07.200000</td>\n",
       "      <td>0 days 00:00:08.900000</td>\n",
       "      <td>0 days 00:00:08.700000</td>\n",
       "      <td>test</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>1438</td>\n",
       "      <td>0 days 01:28:20.781500</td>\n",
       "      <td>0 days 01:28:22.815000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>none</td>\n",
       "      <td>0 days 01:28:20.781500</td>\n",
       "      <td>0 days 01:28:20.832500</td>\n",
       "      <td>0 days 01:28:21.382500</td>\n",
       "      <td>0 days 01:28:22.249000</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>-64.421</td>\n",
       "      <td>545.444910</td>\n",
       "      <td>849.090111</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>1439</td>\n",
       "      <td>0 days 01:28:24.349000</td>\n",
       "      <td>0 days 01:28:27.131500</td>\n",
       "      <td>0 days 01:28:27.066000</td>\n",
       "      <td>train</td>\n",
       "      <td>0 days 01:28:24.349000</td>\n",
       "      <td>0 days 01:28:24.382500</td>\n",
       "      <td>0 days 01:28:24.982500</td>\n",
       "      <td>0 days 01:28:25.532500</td>\n",
       "      <td>0 days 01:28:26.332500</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>756.019</td>\n",
       "      <td>609.255693</td>\n",
       "      <td>537.199998</td>\n",
       "      <td>500.0</td>\n",
       "      <td>44.344417</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>1440</td>\n",
       "      <td>0 days 01:28:27.649000</td>\n",
       "      <td>0 days 01:28:30.465000</td>\n",
       "      <td>0 days 01:28:30.378500</td>\n",
       "      <td>val</td>\n",
       "      <td>0 days 01:28:27.649000</td>\n",
       "      <td>0 days 01:28:27.682500</td>\n",
       "      <td>0 days 01:28:28.299500</td>\n",
       "      <td>0 days 01:28:28.616000</td>\n",
       "      <td>0 days 01:28:29.516000</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>884.761</td>\n",
       "      <td>625.105868</td>\n",
       "      <td>307.792749</td>\n",
       "      <td>500.0</td>\n",
       "      <td>62.098296</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>1441</td>\n",
       "      <td>0 days 01:28:30.981500</td>\n",
       "      <td>0 days 01:28:34.465000</td>\n",
       "      <td>0 days 01:28:34.401500</td>\n",
       "      <td>val</td>\n",
       "      <td>0 days 01:28:30.981500</td>\n",
       "      <td>0 days 01:28:31.149500</td>\n",
       "      <td>0 days 01:28:31.666000</td>\n",
       "      <td>0 days 01:28:32.282500</td>\n",
       "      <td>0 days 01:28:33.382500</td>\n",
       "      <td>...</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1041.113</td>\n",
       "      <td>526.285915</td>\n",
       "      <td>610.483410</td>\n",
       "      <td>500.0</td>\n",
       "      <td>45.017636</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1442</td>\n",
       "      <td>0 days 01:28:34.982000</td>\n",
       "      <td>0 days 01:28:38.215000</td>\n",
       "      <td>0 days 01:28:38.136000</td>\n",
       "      <td>val</td>\n",
       "      <td>0 days 01:28:34.982000</td>\n",
       "      <td>0 days 01:28:35.132500</td>\n",
       "      <td>0 days 01:28:35.799500</td>\n",
       "      <td>0 days 01:28:36.182500</td>\n",
       "      <td>0 days 01:28:37.182500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>972.104</td>\n",
       "      <td>661.462894</td>\n",
       "      <td>367.192338</td>\n",
       "      <td>500.0</td>\n",
       "      <td>56.981867</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1443 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      trial_id             start_time               end_time   \n",
       "0            0        0 days 00:00:00 0 days 00:00:01.700000  \\\n",
       "1            1 0 days 00:00:01.800000 0 days 00:00:03.500000   \n",
       "2            2 0 days 00:00:03.600000 0 days 00:00:05.300000   \n",
       "3            3 0 days 00:00:05.400000 0 days 00:00:07.100000   \n",
       "4            4 0 days 00:00:07.200000 0 days 00:00:08.900000   \n",
       "...        ...                    ...                    ...   \n",
       "1438      1438 0 days 01:28:20.781500 0 days 01:28:22.815000   \n",
       "1439      1439 0 days 01:28:24.349000 0 days 01:28:27.131500   \n",
       "1440      1440 0 days 01:28:27.649000 0 days 01:28:30.465000   \n",
       "1441      1441 0 days 01:28:30.981500 0 days 01:28:34.465000   \n",
       "1442      1442 0 days 01:28:34.982000 0 days 01:28:38.215000   \n",
       "\n",
       "                    go_time  split            fix_on_time   \n",
       "0    0 days 00:00:01.500000   test                    NaT  \\\n",
       "1    0 days 00:00:03.300000   test                    NaT   \n",
       "2    0 days 00:00:05.100000   test                    NaT   \n",
       "3    0 days 00:00:06.900000   test                    NaT   \n",
       "4    0 days 00:00:08.700000   test                    NaT   \n",
       "...                     ...    ...                    ...   \n",
       "1438                    NaT   none 0 days 01:28:20.781500   \n",
       "1439 0 days 01:28:27.066000  train 0 days 01:28:24.349000   \n",
       "1440 0 days 01:28:30.378500    val 0 days 01:28:27.649000   \n",
       "1441 0 days 01:28:34.401500    val 0 days 01:28:30.981500   \n",
       "1442 0 days 01:28:38.136000    val 0 days 01:28:34.982000   \n",
       "\n",
       "                   fix_time         target_on_time             ready_time   \n",
       "0                       NaT                    NaT                    NaT  \\\n",
       "1                       NaT                    NaT                    NaT   \n",
       "2                       NaT                    NaT                    NaT   \n",
       "3                       NaT                    NaT                    NaT   \n",
       "4                       NaT                    NaT                    NaT   \n",
       "...                     ...                    ...                    ...   \n",
       "1438 0 days 01:28:20.832500 0 days 01:28:21.382500 0 days 01:28:22.249000   \n",
       "1439 0 days 01:28:24.382500 0 days 01:28:24.982500 0 days 01:28:25.532500   \n",
       "1440 0 days 01:28:27.682500 0 days 01:28:28.299500 0 days 01:28:28.616000   \n",
       "1441 0 days 01:28:31.149500 0 days 01:28:31.666000 0 days 01:28:32.282500   \n",
       "1442 0 days 01:28:35.132500 0 days 01:28:35.799500 0 days 01:28:36.182500   \n",
       "\n",
       "                   set_time  ...  theta      ts        tp fix_time_dur   \n",
       "0                       NaT  ...    NaN     NaN       NaN          NaN  \\\n",
       "1                       NaT  ...    NaN     NaN       NaN          NaN   \n",
       "2                       NaT  ...    NaN     NaN       NaN          NaN   \n",
       "3                       NaT  ...    NaN     NaN       NaN          NaN   \n",
       "4                       NaT  ...    NaN     NaN       NaN          NaN   \n",
       "...                     ...  ...    ...     ...       ...          ...   \n",
       "1438                    NaT  ...  180.0   900.0   -64.421   545.444910   \n",
       "1439 0 days 01:28:26.332500  ...  180.0   800.0   756.019   609.255693   \n",
       "1440 0 days 01:28:29.516000  ...  180.0   900.0   884.761   625.105868   \n",
       "1441 0 days 01:28:33.382500  ...  180.0  1100.0  1041.113   526.285915   \n",
       "1442 0 days 01:28:37.182500  ...    0.0  1000.0   972.104   661.462894   \n",
       "\n",
       "     target_time_dur    iti  reward_dur  is_outlier  color  position_id  \n",
       "0                NaN    NaN         NaN         NaN   None         None  \n",
       "1                NaN    NaN         NaN         NaN   None         None  \n",
       "2                NaN    NaN         NaN         NaN   None         None  \n",
       "3                NaN    NaN         NaN         NaN   None         None  \n",
       "4                NaN    NaN         NaN         NaN   None         None  \n",
       "...              ...    ...         ...         ...    ...          ...  \n",
       "1438      849.090111  500.0    0.000700        True   None         None  \n",
       "1439      537.199998  500.0   44.344417       False   None         None  \n",
       "1440      307.792749  500.0   62.098296       False   None         None  \n",
       "1441      610.483410  500.0   45.017636       False   None         None  \n",
       "1442      367.192338  500.0   56.981867       False   None         None  \n",
       "\n",
       "[1443 rows x 25 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bf88c029-5e80-4682-b6d3-c16b5578c0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"loading the data\"\"\"\n",
    "\n",
    "data_path = 'data/data_{split}_{bin_sz_ms}ms.pt'\n",
    "train_data = torch.load(data_path.format(split='train', bin_sz_ms=cfg.bin_sz_ms))\n",
    "val_data = torch.load(data_path.format(split='valid', bin_sz_ms=cfg.bin_sz_ms))\n",
    "test_data = torch.load(data_path.format(split='test', bin_sz_ms=cfg.bin_sz_ms))\n",
    "\n",
    "y_valid_obs = val_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_train_obs = train_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "y_test_obs = test_data['y_obs'].type(torch.float32).to(cfg.data_device)\n",
    "n_trials, n_time_bins, n_neurons_obs = y_train_obs.shape\n",
    "n_time_bins_enc = train_data['n_time_bins_enc']\n",
    "\n",
    "y_train_dataset = torch.utils.data.TensorDataset(y_train_obs, )\n",
    "y_val_dataset = torch.utils.data.TensorDataset(y_valid_obs, )\n",
    "y_test_dataset = torch.utils.data.TensorDataset(y_test_obs, )\n",
    "train_dataloader = torch.utils.data.DataLoader(y_train_dataset, batch_size=cfg.batch_sz, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(y_val_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(y_test_dataset, batch_size=y_valid_obs.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "693ef513-8702-4d22-ba2e-e3747747dfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"likelihood pdf\"\"\"\n",
    "H = utils.ReadoutLatentMask(cfg.n_latents, cfg.n_latents_read)\n",
    "readout_fn = nn.Sequential(H, nn.Linear(cfg.n_latents_read, n_neurons_obs))\n",
    "readout_fn[-1].bias.data = prob_utils.estimate_poisson_rate_bias(train_dataloader, cfg.bin_sz)\n",
    "likelihood_pdf = PoissonLikelihood(readout_fn, n_neurons_obs, cfg.bin_sz, device=cfg.device)\n",
    "\n",
    "\"\"\"dynamics module\"\"\"\n",
    "Q_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "dynamics_fn = utils.build_gru_dynamics_function(cfg.n_latents, cfg.n_hidden_dynamics, device=cfg.device)\n",
    "dynamics_mod = DenseGaussianDynamics(dynamics_fn, cfg.n_latents, Q_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"initial condition\"\"\"\n",
    "m_0 = torch.zeros(cfg.n_latents, device=cfg.device)\n",
    "Q_0_diag = 1. * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "initial_condition_pdf = DenseGaussianInitialCondition(cfg.n_latents, m_0, Q_0_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"local/backward encoder\"\"\"\n",
    "backward_encoder = BackwardEncoderLRMvn(cfg.n_latents, cfg.n_hidden_backward, cfg.n_latents,\n",
    "                                        rank_local=cfg.rank_local, rank_backward=cfg.rank_backward,\n",
    "                                        device=cfg.device)\n",
    "local_encoder = LocalEncoderLRMvn(cfg.n_latents, n_neurons_obs, cfg.n_hidden_local, cfg.n_latents, rank=cfg.rank_local,\n",
    "                                  device=cfg.device, dropout=cfg.p_local_dropout)\n",
    "nl_filter = NonlinearFilter(dynamics_mod, initial_condition_pdf, device=cfg.device)\n",
    "\n",
    "\"\"\"sequence vae\"\"\"\n",
    "ssm = LowRankNonlinearStateSpaceModel(dynamics_mod, likelihood_pdf, initial_condition_pdf, backward_encoder,\n",
    "                                      local_encoder, nl_filter, device=cfg.device)\n",
    "\n",
    "\"\"\"lightning\"\"\"\n",
    "#seq_vae = LightningDMFCRSG.load_from_checkpoint('ckpts/smoother/acausal/epoch=997_valid_loss=3288.73_valid_bps_enc=0.61_valid_bps_bhv=0.12.ckpt',\n",
    "#                                                ssm=ssm, cfg=cfg, n_time_bins_enc=n_time_bins_enc, n_time_bins_bhv=n_bins_bhv, strict=False)\n",
    "# seq_vae = LightningDMFCRSG(ssm, cfg, n_time_bins_enc, n_bins_bhv)\n",
    "seq_vae = LightningDMFCRSG(ssm, cfg, n_time_bins_enc, n_bins_bhv)\n",
    "csv_logger = CSVLogger('logs/smoother/acausal/', name=f'sd_{cfg.seed}_r_y_{cfg.rank_local}_r_b_{cfg.rank_backward}', version='smoother_acausal')\n",
    "ckpt_callback = ModelCheckpoint(save_top_k=3, monitor='valid_bps_enc', mode='max', dirpath='ckpts/smoother/acausal/', save_last=True,\n",
    "                                filename='{epoch:0}_{valid_loss:0.2f}_{valid_bps_enc:0.2f}_{valid_bps_bhv:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216cff8-1cbe-4fd3-9038-c5087ed57395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[rank: 0] Seed set to 1239\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/mahmoud/catnip/xfads/workshop/monkey_timing/ckpts/smoother/acausal exists and is not empty.\n",
      "\n",
      "  | Name | Type                            | Params\n",
      "---------------------------------------------------------\n",
      "0 | ssm  | LowRankNonlinearStateSpaceModel | 478 K \n",
      "---------------------------------------------------------\n",
      "478 K     Trainable params\n",
      "0         Non-trainable params\n",
      "478 K     Total params\n",
      "1.915     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]current_epoch: 0\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/xfads/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  33%|███▎      | 1/3 [04:30<09:00,  0.00it/s, v_num=usal]"
     ]
    }
   ],
   "source": [
    "trainer = lightning.Trainer(max_epochs=cfg.n_epochs,\n",
    "                            gradient_clip_val=1.0,\n",
    "                            default_root_dir='lightning/',\n",
    "                            callbacks=[ckpt_callback],\n",
    "                            logger=csv_logger,\n",
    "                            strategy='ddp_notebook',\n",
    "                            accelerator='cpu',\n",
    "                            )\n",
    "\n",
    "trainer.fit(model=seq_vae, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "torch.save(ckpt_callback.best_model_path, 'ckpts/smoother/acausal/best_model_path.pt')\n",
    "trainer.test(dataloaders=test_dataloader, ckpt_path='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da8d46-43cc-4f8f-ab84-127e0803f05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfads",
   "language": "python",
   "name": "xfads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
