{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd5c146-ac98-4c7e-a850-d049ba8e73dc",
   "metadata": {},
   "source": [
    "# eXponential Family Dynamical Systems (XFADS): Large-scale nonlinear Gaussian state-space modeling\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/catniplab/xfads/blob/workshop/workshop/vanderpol/main.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<br>Now, to add a bit of non-linearity, we apply XFADS to synthesized data from the Vanderpol oscillator. Although, there is no big difference, technically, from the linear dynamical system example, the posterior distribution of the latents inferred from the Vanderpol oscillations is more complex than the one of those generated from the linear dynamical system in [the previous example](https://github.com/catniplab/xfads/blob/workshop/workshop/linear_ds/main.ipynb).<br>\n",
    "If you feel comfortable dabbling with XFADS by directly applying it to neural data, you can jump to applying it to the `mc_maze` dataset at the [`monkey_reaching` example](https://github.com/catniplab/xfads/blob/workshop/workshop/monkey_reaching/main.ipynb).<br>\n",
    "\n",
    "For an in-depth understanding of the underlying theory of this statistical framework, dive into the paper: [Dowling, Zhao, Park. 2024](https://arxiv.org/abs/2403.01371). But if you are more into practicality and neural data analysis, it's enough to start with the examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e963ed2-34f1-480a-8622-5c5197a015f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installation\n",
    "After cloning the repo, and installing anaconda, or miniconda, create the environment by running:<br>\n",
    "`conda env create -f environment.yaml`<br>\n",
    "\n",
    "Add the `xfads` package to the `PYTHONPATH` of the environment<br>\n",
    "`pip install -e .`<br>\n",
    "\n",
    "If you are on Google Colab, which is recommended (just make sure to zip and download the trained models!), run the following cell to clone the repo and install the requirements, and you are ready to go!<br>\n",
    "(Since Colab uses sessions anyway, it won't be that useful to use an environment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9c725-e8bc-4c27-b248-9dbd9a406a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if is_running_in_colab():\n",
    "    !git clone https://github.com/catniplab/xfads.git\n",
    "    %cd xfads\n",
    "    # install the dependencies\n",
    "    !pip install torch pytorch-lightning scikit-learn hydra-core matplotlib einops\n",
    "    # wrap the XFADS package\n",
    "    !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131a33b-7cc6-41d6-b6fc-d755642a2e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import hydra\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as lightning\n",
    "\n",
    "import xfads.utils as utils\n",
    "import xfads.plot_utils as plot_utils\n",
    "\n",
    "from matplotlib import cm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from hydra import compose, initialize\n",
    "\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from vdp_utils import *\n",
    "\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianDynamics\n",
    "from xfads.ssm_modules.likelihoods import GaussianLikelihood\n",
    "from xfads.smoothers.lightning_trainers import LightningNonlinearSSM\n",
    "from xfads.ssm_modules.dynamics import DenseGaussianInitialCondition\n",
    "from xfads.ssm_modules.encoders import LocalEncoderLRMvn, BackwardEncoderLRMvn\n",
    "from xfads.smoothers.lightning_trainers import LightningNonlinearSSM, LightningDMFCRSG\n",
    "from xfads.smoothers.nonlinear_smoother_causal import NonlinearFilter, LowRankNonlinearStateSpaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3c772-017c-44ca-af1c-64c77d253e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319792c-a51f-4057-ad7b-e0743d42e31a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2919973-482b-47a4-8f57-cfdbd79fa3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"config\"\"\"\n",
    "\n",
    "cfg = {\n",
    "    # --- graphical model --- #\n",
    "    'n_latents': 2,\n",
    "    'n_latents_read': 2,\n",
    "\n",
    "    'rank_local': 2,\n",
    "    'rank_backward': 2,\n",
    "\n",
    "    'Q_init': 1e-1,\n",
    "    'n_hidden_dynamics': 64,\n",
    "\n",
    "    # --- inference network --- #\n",
    "    'n_samples': 5,\n",
    "    'n_hidden_local': 128,\n",
    "    'n_hidden_backward': 64,\n",
    "\n",
    "    # --- hyperparameters --- #\n",
    "    'use_cd': False,\n",
    "    'p_mask_a': 0.0,\n",
    "    'p_mask_b': 0.0,\n",
    "    'p_mask_apb': 0.0,\n",
    "    'p_mask_y_in': 0.0,\n",
    "    'p_local_dropout': 0.4,\n",
    "    'p_backward_dropout': 0.0,\n",
    "\n",
    "    # --- training --- #\n",
    "    'device': 'cpu',\n",
    "    'data_device': 'cpu',\n",
    "\n",
    "    'lr': 1e-3,\n",
    "    'n_epochs': 1500,\n",
    "    'batch_sz': 128,\n",
    "\n",
    "    # --- misc --- #\n",
    "    'bin_sz': 20e-3,\n",
    "    'bin_sz_ms': 20,\n",
    "    'n_bins_bhv': 55,\n",
    "\n",
    "    'seed': 1234,\n",
    "    'default_dtype': torch.float32,\n",
    "\n",
    "    # --- ray --- #\n",
    "    'n_ray_samples': 10,\n",
    "}\n",
    "\n",
    "class DictAsAttributes(dict):\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self:\n",
    "            return self[attr]\n",
    "        else:\n",
    "            raise AttributeError(f\"'DictAsAttributes' object has no attribute '{attr}'\")\n",
    "\n",
    "cfg = DictAsAttributes(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1637ae-dfa0-4de4-87d6-2e4b03dfb213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lightning.seed_everything(cfg.seed, workers=True)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722780a5-80ea-4fd7-974b-04c6a42857f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generating the data\n",
    "For this example, we will use data synthesized from the Vanderpol oscillator, which is a classic mathematical model that describes self-sustained oscillations observed in various physical systems. Then we will infer the posterior distribution of the latent factors that govern the low-dimensional underlying dynamics of the data, by learning a graphical state space model of the principal latent and the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf497be7-6b6b-486f-97cb-03e593d9d5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n_trials x n_time_bins x n_neurons\n",
    "\n",
    "n_trials = 500\n",
    "n_neurons = 100\n",
    "n_time_bins = 250\n",
    "\n",
    "mean_fn = utils.VdpDynamicsModel()\n",
    "C = utils.FanInLinear(cfg.n_latents, n_neurons, device=cfg.device).requires_grad_(False)\n",
    "Q_diag = 1e-2 * torch.ones(cfg.n_latents, device=cfg.device)\n",
    "Q_0_diag = torch.ones(cfg.n_latents, device=cfg.device)\n",
    "R_diag = 1e-1 * torch.ones(n_neurons, device=cfg.device)\n",
    "m_0 = torch.zeros(cfg.n_latents, device=cfg.device)\n",
    "\n",
    "z = utils.sample_gauss_z(mean_fn, Q_diag, m_0, Q_0_diag, n_trials, n_time_bins)\n",
    "y = C(z) + torch.sqrt(R_diag) * torch.randn((n_trials, n_time_bins, n_neurons), device=cfg.device)\n",
    "y = y.detach()\n",
    "\n",
    "y_train, z_train = y[:n_trials//2], z[:n_trials//2]\n",
    "y_valid, z_valid = y[n_trials//2:], z[n_trials//2:]\n",
    "\n",
    "y_train_dataset = torch.utils.data.TensorDataset(y_train,)\n",
    "y_valid_dataset = torch.utils.data.TensorDataset(y_valid,)\n",
    "train_dataloader = torch.utils.data.DataLoader(y_train_dataset, batch_size=cfg.batch_sz, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(y_valid_dataset, batch_size=cfg.batch_sz, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f67290-f28e-4710-9564-916d061122f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indices of n example trials\n",
    "n_ex_trial = 4\n",
    "ex_trials = np.random.choice(range(0, y_valid.shape[0]), size=n_ex_trial, replace=False)\n",
    "\n",
    "# indices of n example neuron\n",
    "n_ex_neurons = 4\n",
    "ex_neurons = np.random.choice(range(0, y_valid.shape[2]), size=4, replace=False)\n",
    "\n",
    "# top n neurons to plot wihen visualizing trials\n",
    "top_n_neurons = n_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ad05b-66b3-4ee8-b468-e84b0e0aa60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_rastor(y_valid, ex_trials, top_n_neurons, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f363b8-4d8e-48ed-acaf-b8156030aae0",
   "metadata": {},
   "source": [
    "## Structuring the state-space model\n",
    "The modules of the XFADS package are organized in a modular way, allowing the users to change and plug in their own definitions of the classes that structure the elements of the model, i.e. the dynamics function, the likelihood density, the amortization network, etc.\n",
    "\n",
    "The configuration depends on the problem - `dynamics_mod`, `initial_c_pdf`, `likelihood_pdf`, `local_encoder`, and `backward_encoder` can be configured as desired. We include some general classes in `ssm_modules/encoders`, `ssm_modules/likelihoods`, and `ssm_modules/dynamics` that should be sufficient for a wide range of problems.  Below is an example configuration.\n",
    "\n",
    "In each iteration of the variational inference, we need to optimize the parameters of the approximate posterior, which can be quite inefficient, to enable smortized inference for the state-space model, we follow the technique of using a trainable NN, known as an **inference network** or **recognition network**, to predict these parameters from the observed data.\n",
    "\n",
    "A possible drawback of the traditional inference frameworks is that missing observations obstruct inference, i.e. cannot naturally accommodate missing data. But in a state-space graphical model, the latent state posterior should be accessible for every time point even with missing observations. To enable the amortized inference network to process missing observations in a principled way, we decompose the natural parameter update into two additive components: i) a **local encoder**, for current observation, and ii) a **backward encoder**, for future observations.\n",
    "\n",
    "For a detailed step-by-step development of XFADS, check the Method section of [the paper](https://arxiv.org/abs/2403.01371)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d697a34-34ad-41d5-86a1-d6f452bf296f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"likelihood pdf\"\"\"\n",
    "likelihood_pdf = GaussianLikelihood(C, n_neurons, R_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"dynamics module\"\"\"\n",
    "dynamics_fn = utils.build_gru_dynamics_function(cfg.n_latents, cfg.n_hidden_dynamics, device=cfg.device)\n",
    "dynamics_mod = DenseGaussianDynamics(dynamics_fn, cfg.n_latents, Q_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"initial condition\"\"\"\n",
    "initial_condition_pdf = DenseGaussianInitialCondition(cfg.n_latents, m_0, Q_0_diag, device=cfg.device)\n",
    "\n",
    "\"\"\"local/backward encoder\"\"\"\n",
    "backward_encoder = BackwardEncoderLRMvn(cfg.n_latents, cfg.n_hidden_backward, cfg.n_latents,\n",
    "                                        rank_local=cfg.rank_local, rank_backward=cfg.rank_backward,\n",
    "                                        device=cfg.device)\n",
    "local_encoder = LocalEncoderLRMvn(cfg.n_latents, n_neurons, cfg.n_hidden_local, cfg.n_latents, rank=cfg.rank_local,\n",
    "                                  device=cfg.device, dropout=cfg.p_local_dropout)\n",
    "nl_filter = NonlinearFilter(dynamics_mod, initial_condition_pdf, device=cfg.device)\n",
    "\n",
    "\"\"\"sequence vae\"\"\"\n",
    "ssm = LowRankNonlinearStateSpaceModel(dynamics_mod, likelihood_pdf, initial_condition_pdf, backward_encoder,\n",
    "                                      local_encoder, nl_filter, device=cfg.device)\n",
    "\n",
    "\"\"\"lightning\"\"\"\n",
    "seq_vae = LightningNonlinearSSM(ssm, cfg)\n",
    "# seq_vae = LightningNonlinearSSM.load_from_checkpoint('ckpts/epoch=471_valid_loss=14513.6044921875.ckpt', ssm=ssm, cfg=cfg)\n",
    "\n",
    "csv_logger = CSVLogger('logs/', name=f'r_y_{cfg.rank_local}_r_b_{cfg.rank_backward}', version='noncausal')\n",
    "ckpt_callback = ModelCheckpoint(save_top_k=3, monitor='valid_loss', mode='min', dirpath='ckpts/',\n",
    "                                filename='{epoch:0}_{valid_loss}')\n",
    "\n",
    "# Show model structure\n",
    "seq_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c7869-be0d-44ee-8f59-83eae9d54155",
   "metadata": {},
   "source": [
    "## Training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3785a-edc7-450a-a3e7-2a6584d223f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = lightning.Trainer(max_epochs=cfg.n_epochs,\n",
    "                            gradient_clip_val=1.0,\n",
    "                            default_root_dir='lightning/',\n",
    "                            callbacks=[ckpt_callback],\n",
    "                            logger=csv_logger\n",
    "                            )\n",
    "\n",
    "trainer.fit(model=seq_vae, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "torch.save(ckpt_callback.best_model_path, 'ckpts/best_model_path.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126e249-e548-4f3b-ad2a-d8587d047920",
   "metadata": {},
   "source": [
    "## Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4761863-9bce-4619-a720-b030af0cd743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_vae = LightningNonlinearSSM.load_from_checkpoint('ckpts/epoch=698_valid_loss=24543.8125.ckpt', ssm=ssm, cfg=cfg)\n",
    "\n",
    "loss, z, stats = seq_vae.ssm(y_valid, cfg.n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868ba73-1f27-4481-8e6a-f8d66a239663",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "First, let's compare the latents inferred by XFADS from the data, with the original two factors those  dynamical system that was used to generate the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539bc38-162d-4812-a3f9-9b9984757485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_latent_trajectory(z_valid, stats['m_f'], ex_trials, latent_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c2040-1fce-48da-b643-2dad23883c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_latent_trajectory(z_valid, stats['m_s'], ex_trials, latent_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29388b0-57e6-43f2-af84-9c7605e0b116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "plot_utils.plot_two_d_vector_field(seq_vae.ssm.dynamics_mod.mean_fn, axs)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75110c1-1de9-45b7-9376-710aca4bcbb7",
   "metadata": {},
   "source": [
    "## Filtering, Smoothing, and Prediction\n",
    "There is a trick we worked on to conceptualize the approximate smoothing problem as an approximate filtering problem for pseudo-observations that encode a representation of current and future data.\n",
    "\n",
    "We draw inspiration from a quintessential facet of conjugate Bayesian inference: natural parameters of the posterior are a sum-separable combination of the natural parameters of the prior in addition to a data-dependent term. The latter can now be thought of as a data-dependent update to the prior for a conjugate pseudo-observation.\n",
    "\n",
    "Importantly, pseudo-observations defined this way encode the current *and* future observations of the raw data – an essential component for transforming the statistical smoothing problem into an alternative filtering problem.\n",
    "\n",
    "The next figure illustrates the smoothing and predictive performance of XFADS trained on the data generated from the Vanderpol oscillator. To the left of the red line are samples from the posterior during the data window, and to the right of the red line are samples unrolled from $p_{\\theta}(z_t | z_{t-1})$.\n",
    "\n",
    "So, we end up with three regimes:\n",
    "**Filtering** `stats['m_f']`, **Smoothing** `stats['m_s']`, and **Prediction** `stats['m_p']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c17b0-f408-40b1-9ea0-3aa506fcf611",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    fig, axs = plt.subplots(len(ex_trials), 1, figsize=(6, 8))\n",
    "    default_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    \n",
    "    plot_z_2d(fig, axs, ex_trials, stats['m_f'][ex_trials], color=default_colors[0], regime='filtering')\n",
    "    plot_z_2d(fig, axs, ex_trials, stats['m_p'][ex_trials], color=default_colors[1], regime='prediction')\n",
    "    \n",
    "    fig.suptitle('filtered vs predicted trajectories\\n\\n\\n\\n')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.47, 0.94), ncol=1, fontsize=6)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76be0bf-1114-48d3-8721-b15e40bdbe05",
   "metadata": {},
   "source": [
    "## Generating corresponding observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b0e92-220c-457e-902f-0ca8cf9f18de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# smoothed and predicted log rates\n",
    "r_s = seq_vae.ssm.to(cfg.data_device).likelihood_pdf.readout_fn(stats['m_s'])\n",
    "r_p = seq_vae.ssm.to(cfg.data_device).likelihood_pdf.readout_fn(stats['m_p'])\n",
    "\n",
    "# sampeling from the rates log possion to get the spike counts\n",
    "y_s = torch.poisson(cfg.bin_sz * torch.exp(r_s))\n",
    "y_p = torch.poisson(cfg.bin_sz * torch.exp(r_p))\n",
    "\n",
    "y_hat = torch.cat([y_s[:, :, :], y_p], dim=2)\n",
    "z_hat = torch.cat([stats['m_s'][:, :, :], stats['m_p']], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b66d56-0b47-4a1f-983d-4e2be8b5d822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " with torch.no_grad():\n",
    "        \n",
    "    n_ex_neurons = 9\n",
    "    ex_neurons = np.random.choice(range(0, y_valid.shape[2]), size=4, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(int(np.sqrt(n_ex_neurons)), int(np.sqrt(n_ex_neurons)), figsize=(9, 5))\n",
    "    fig.suptitle('trial-averaged unit activity\\n\\n')\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        \n",
    "        x = np.arange(n_time_bins) * cfg.bin_sz_ms\n",
    "        y_data = torch.tensor(gaussian_filter1d(torch.mean(y_valid[:, :, ex_neurons[i]], axis=0), sigma=4, axis=0))\n",
    "        y_model = torch.tensor(gaussian_filter1d(torch.mean(r_s[:, :, ex_neurons[i]], axis=0), sigma=4, axis=0))\n",
    "                         \n",
    "        ax.plot(x, y_data, label='data' if i == 0 else '')\n",
    "        ax.plot(x, y_model, label='model' if i == 0 else '')\n",
    "\n",
    "        ax.set_title(f'unit {ex_neurons[i]}', fontsize=8)\n",
    "        ax.set_xlabel('time (ms)' if i == int(np.sqrt(n_ex_neurons)) else '')\n",
    "        ax.tick_params(axis='x', labelsize=8)\n",
    "        ax.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 0.94), ncol=1, fontsize=7)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897de74-1d8e-4955-abf8-1e4366dbcdde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfads",
   "language": "python",
   "name": "xfads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
